<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Paper-info 汇总和记录-updating | Ryanran&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="前言 &amp;emsp; &amp;emsp;主要对平时所读大模型、NLG相关paper、观点和信息进行汇总，实时更新。 记录  HuggingGPT: Solving AI Tasks with ChatGPT in Hugging Face http:&#x2F;&#x2F;t.km&#x2F;qaodgc 核心用ChatGPT解析意图（任务规划）、模型选择、在专家模型执行任务后（任务执行），根据返回结果利用ChatGPT生成响应（响应">
<meta property="og:type" content="article">
<meta property="og:title" content="Paper-info 汇总和记录-updating">
<meta property="og:url" content="https://ryanran92.github.io/2022/11/08/Paper-info%E6%B1%87%E6%80%BB%E5%92%8C%E8%AE%B0%E5%BD%95-updating/index.html">
<meta property="og:site_name" content="Ryanran&#39;s Blog">
<meta property="og:description" content="前言 &amp;emsp; &amp;emsp;主要对平时所读大模型、NLG相关paper、观点和信息进行汇总，实时更新。 记录  HuggingGPT: Solving AI Tasks with ChatGPT in Hugging Face http:&#x2F;&#x2F;t.km&#x2F;qaodgc 核心用ChatGPT解析意图（任务规划）、模型选择、在专家模型执行任务后（任务执行），根据返回结果利用ChatGPT生成响应（响应">
<meta property="og:locale">
<meta property="og:image" content="https://ryanran92.github.io/2022/11/08/Paper-info%E6%B1%87%E6%80%BB%E5%92%8C%E8%AE%B0%E5%BD%95-updating/p0412_01.png">
<meta property="og:image" content="https://ryanran92.github.io/2022/11/08/Paper-info%E6%B1%87%E6%80%BB%E5%92%8C%E8%AE%B0%E5%BD%95-updating/p0412_02.png">
<meta property="og:image" content="https://ryanran92.github.io/2022/11/08/Paper-info%E6%B1%87%E6%80%BB%E5%92%8C%E8%AE%B0%E5%BD%95-updating/p1.png">
<meta property="og:image" content="https://ryanran92.github.io/2022/11/08/Paper-info%E6%B1%87%E6%80%BB%E5%92%8C%E8%AE%B0%E5%BD%95-updating/p2.png">
<meta property="og:image" content="https://ryanran92.github.io/2022/11/08/Paper-info%E6%B1%87%E6%80%BB%E5%92%8C%E8%AE%B0%E5%BD%95-updating/p3.png">
<meta property="og:image" content="https://ryanran92.github.io/2022/11/08/Paper-info%E6%B1%87%E6%80%BB%E5%92%8C%E8%AE%B0%E5%BD%95-updating/p4.png">
<meta property="og:image" content="https://ryanran92.github.io/2022/11/08/Paper-info%E6%B1%87%E6%80%BB%E5%92%8C%E8%AE%B0%E5%BD%95-updating/p5.png">
<meta property="og:image" content="https://ryanran92.github.io/2022/11/08/Paper-info%E6%B1%87%E6%80%BB%E5%92%8C%E8%AE%B0%E5%BD%95-updating/p6.png">
<meta property="og:image" content="https://ryanran92.github.io/2022/11/08/Paper-info%E6%B1%87%E6%80%BB%E5%92%8C%E8%AE%B0%E5%BD%95-updating/p7.png">
<meta property="og:image" content="https://ryanran92.github.io/2022/11/08/Paper-info%E6%B1%87%E6%80%BB%E5%92%8C%E8%AE%B0%E5%BD%95-updating/p8.png">
<meta property="og:image" content="https://ryanran92.github.io/2022/11/08/Paper-info%E6%B1%87%E6%80%BB%E5%92%8C%E8%AE%B0%E5%BD%95-updating/p9.png">
<meta property="og:image" content="https://ryanran92.github.io/2022/11/08/Paper-info%E6%B1%87%E6%80%BB%E5%92%8C%E8%AE%B0%E5%BD%95-updating/p10.png">
<meta property="article:published_time" content="2022-11-07T16:00:00.000Z">
<meta property="article:modified_time" content="2023-04-13T07:16:48.365Z">
<meta property="article:author" content="Ryanran">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ryanran92.github.io/2022/11/08/Paper-info%E6%B1%87%E6%80%BB%E5%92%8C%E8%AE%B0%E5%BD%95-updating/p0412_01.png">
  
    <link rel="alternate" href="/atom.xml" title="Ryanran's Blog" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Ryanran&#39;s Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Buscar"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Buscar"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://ryanran92.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-Paper-info汇总和记录-updating" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/11/08/Paper-info%E6%B1%87%E6%80%BB%E5%92%8C%E8%AE%B0%E5%BD%95-updating/" class="article-date">
  <time class="dt-published" datetime="2022-11-07T16:00:00.000Z" itemprop="datePublished">2022-11-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Paper-info 汇总和记录-updating
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><hr>
<p>&emsp; &emsp;主要对平时所读大模型、NLG相关paper、观点和信息进行汇总，实时更新。</p>
<h1 id="记录"><a href="#记录" class="headerlink" title="记录"></a>记录</h1><hr>
<ul>
<li><strong>HuggingGPT:</strong> Solving AI Tasks with ChatGPT in Hugging Face <a target="_blank" rel="noopener" href="http://t.km/qaodgc" title="https://arxiv.org/pdf/2303.17580.pdf">http://t.km/qaodgc</a> 核心用ChatGPT解析意图（任务规划）、模型选择、在专家模型执行任务后（任务执行），根据返回结果利用ChatGPT生成响应（响应生成），设计为一超长的Prompt（见下图1），所以缺点是暂时来说成本很高，优点是利用了很多专家模型。 Time: 2023-04-12 <br></li>
</ul>
 <img src="/2022/11/08/Paper-info汇总和记录-updating/p0412_01.png" width=65%>
 

 <img src="/2022/11/08/Paper-info汇总和记录-updating/p0412_02.png" width=65%>
<br>

<ul>
<li><p>  <strong>Sparks of AGI一作演讲</strong> <a target="_blank" rel="noopener" href="https://hub.baai.ac.cn/view/25373%EF%BC%8C">https://hub.baai.ac.cn/view/25373，</a> 整体和论文内容一致，对比各项任务中GPT-4的智能，核心观点很有趣：即如何定义GPT-4是否有智能，按照不同维度可不同定义：智力包括推理、计划、解决问题、抽象思维、比较复杂的观点以及快速学习和从经验中学习等能力，GPT-4无法计划、缺乏记忆无法实时学习，其他能力可被定义为AGI。 Time: 2023-04-12<br></p>
</li>
<li><p>  <strong>AutoGPT</strong> <a target="_blank" rel="noopener" href="https://github.com/torantulino/auto-gpt">https://github.com/torantulino/auto-gpt</a> 基于GPT-4/3.5 的实验性开源应用程序，相当于给GPT大脑一个内存和身体，设定任务后让其自己解决问题，同时可互联网访问、长期和短期内存管理、文件存储和生成摘要等，在其基础上构造垂类X-GPT，想象空间较大，但未看出整体设置的必要性；类似新应用可参照<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/621132445">https://zhuanlan.zhihu.com/p/621132445</a> Time: 2023-04-12<br></p>
</li>
</ul>
<ol>
<li><p><strong>针对多步推理进行小型语言模型的专门化</strong>  <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2301.12726.pdf">https://arxiv.org/pdf/2301.12726.pdf</a>  <a target="_blank" rel="noopener" href="https://hub.baai.ac.cn/view/25238">https://hub.baai.ac.cn/view/25238</a> 之前介绍中提到的第四步模型专业化，以专门针对目标任务专门化模型能力，即将大模型能力集中在特定的目标任务上，使用较小模型多步推理来靠近大模型的涌现能力。另外文中提出了包括数据格式、起始模型等选择方法。 Time: 2023-04-06</p>
</li>
<li><p><strong>LLM-Adapters</strong>  <a target="_blank" rel="noopener" href="https://hub.baai.ac.cn/view/25288">https://hub.baai.ac.cn/view/25288</a>  <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2304.01933.pdf">https://arxiv.org/pdf/2304.01933.pdf</a> LLM的局部微调（PEFT）方法，将LLaMa和三种LLM-adapter集成（序列Adapter、平行Adapter和LoRA），在较小规模的LLMs（7B）中使用基于适配器的PEFT，在简单的数学推理数据集的零次推理中，产生了与强大的LLMs（175B）相当的性能。   Time: 2023-04-06 </p>
 <img src="/2022/11/08/Paper-info汇总和记录-updating/p1.png" width=65%>
<span id="more"></span>    </li>
<li><p><strong>AI 将改变一切</strong>  <a target="_blank" rel="noopener" href="https://hub.baai.ac.cn/view/25313">https://hub.baai.ac.cn/view/25313</a> 观点输出文，整篇为翻译文，有些点翻译的较奇怪，提出一系列有趣的观点：</p>
<img src="/2022/11/08/Paper-info汇总和记录-updating/p2.png" width=65%></li>
<li><p><strong>BloombergGPT: A Large Language Model for Finance</strong>  <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2303.17564.pdf">https://arxiv.org/pdf/2303.17564.pdf</a> 构建和训练了专门用于金融领域的LLM，开发了拥有500亿参数的语言模型——BloombergGPT，核心是利用五项金融任务在Bloom基础上构建模型，参照Chinchilla和现有数据，决定构建50B模型，效果超出Bloom，但未和ChatGPT比较。</p>
</li>
<li><p>ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks  <a target="_blank" rel="noopener" href="https://hub.baai.ac.cn/view/25112">https://hub.baai.ac.cn/view/25112</a>  <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.15056">https://arxiv.org/abs/2303.15056</a> ChatGPT在包括相关性、立场、主题和两种框架检测任务中的性能超过人工众包工人，在五分之四的任务上，ChatGPT 的零样本准确率高于 MTurk，且成本便宜20倍。</p>
</li>
</ol>
<img src="/2022/11/08/Paper-info汇总和记录-updating/p3.png" width=65%>

<ol start="6">
<li><strong>MEMORIZING TRANSFORMERS</strong>  <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.08913.pdf">https://arxiv.org/pdf/2203.08913.pdf</a>   <a target="_blank" rel="noopener" href="https://hub.baai.ac.cn/view/24957%EF%BC%8C%E5%B0%9D%E8%AF%95%E5%A2%9E%E5%8A%A0%E2%80%9C%E8%AE%B0%E5%BF%86%E2%80%9D%E6%9C%BA%E5%88%B6%EF%BC%8C%E6%8F%90%E5%87%BA%E4%BA%86kNN-augmented">https://hub.baai.ac.cn/view/24957，尝试增加“记忆”机制，提出了kNN-augmented</a> attention，通过使用k-nearest-neighbor lookup外部存储器，增加模型可关注的上下文长度。文中包括从头预训练，或在预训练模型基础上增加记忆，但效率未提及，KNN-LM效率是个问题。</li>
</ol>
<img src="/2022/11/08/Paper-info汇总和记录-updating/p4.png" width=65%>


<ol start="7">
<li><p><strong>为什么现在的LLM都是Decoder-only的架构</strong>？  <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/ZsHX-M9pisUvG9vqfzdzTQ">https://mp.weixin.qq.com/s/ZsHX-M9pisUvG9vqfzdzTQ</a> 苏剑林提出：LLM 之所以主要都用 Decoder-only 架构，除了训练效率和工程实现上的优势外，在理论上是因为 Encoder 的双向注意力会存在低秩问题，在同等参数量、同等推理成本下，Decoder-only 架构就是最优选择。没太看懂，需要思考。</p>
</li>
<li><p><strong>Lilian Weng新博文：关于提示工程的介绍</strong> <a target="_blank" rel="noopener" href="http://t.km/2ssfg0" title="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/">http://t.km/2ssfg0</a> <a target="_blank" rel="noopener" href="http://t.km/4r1s5f" title="https://hub.baai.ac.cn/view/24941">http://t.km/4r1s5f</a> 关于Prompt Engineering的汇总介绍，包括基础提升工程、指令提示、CoT、自动提示、知识增强（包括外部调用、检索等）等Prompting，详细内容会借鉴并更新在《大型语言模型(LLM)的使用和思考》中。</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://hub.baai.ac.cn/view/24740">https://hub.baai.ac.cn/view/24740</a> ，提出<strong>对于涌现能力的分析和观点</strong>，1是涌现能力在物理学、进化生物学等自然科学中同样存在，如物理学中水的固化；2是涌现能力可能和评价方式有关，比如只评价目标没有过程指标时，可能会凸显涌现能力，但其实是渐进式发展的；3是讨论模型是否可以做到更大，结论是受数据所限。</p>
</li>
<li><p><strong>用大语言模型GPT-3直接替代传统搜索引擎</strong>： <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2209.10063">https://arxiv.org/abs/2209.10063</a> ， <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/JDJ0JdW77NyRaWRFH2Jdkw">https://mp.weixin.qq.com/s/JDJ0JdW77NyRaWRFH2Jdkw</a> ，核心即先生成再阅读（Generate-then-Read），首先向大型语言模型提示（prompt）生成基于给定问题的上下文文档，然后阅读生成的文档以产生最终答案，和基于检索增强的LM思想一致。</p>
</li>
</ol>
<img src="/2022/11/08/Paper-info汇总和记录-updating/p5.png" width=65%>

<ol start="11">
<li><strong>Guiding Large Language Models via Directional Stimulus Prompting</strong>  <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2302.11520.pdf">https://arxiv.org/pdf/2302.11520.pdf</a> 使用policy LM生成一个stimulus（一组关键词），即加了一步中间过程：原文-关键词-输出，关键词这里叫stimulus（刺激），剩余方式和ChatGPT一致，SFT-&gt;RL。</li>
</ol>
<img src="/2022/11/08/Paper-info汇总和记录-updating/p6.png" width=65%>


<ol start="12">
<li><strong>The Capacity for Moral Self-Correction in Large Language Models</strong>  <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2302.07459.pdf">https://arxiv.org/pdf/2302.07459.pdf</a> 用“魔法”对抗“魔法”，核心给大模型增加新的指令(IF)，新的思考时间(CoT)来引导大模型减少有害输出，让其自我修正，在实际应用中，也可以增加相关的Prompt，很大概率可以解决，但非根本解决方案。</li>
<li><strong>Why did all of the public reproduction of GPT-3 fail</strong>?  <a target="_blank" rel="noopener" href="https://hub.baai.ac.cn/view/24306">https://hub.baai.ac.cn/view/24306</a>  <a target="_blank" rel="noopener" href="https://jingfengyang.github.io/gpt">https://jingfengyang.github.io/gpt</a> ，写的很好很详尽的一篇文字，解释其他大模型无法复现GPT3的原因，同时给出使用LLM的decision tree，非无脑使用，很多时候在确定任务集，使用微调百亿模型即可达到SOTA。</li>
</ol>
<img src="/2022/11/08/Paper-info汇总和记录-updating/p7.png" width=65%>

<ol start="14">
<li><p><strong>ChatGPT必读论文、博客和API工具（含中文指南）</strong>： <a target="_blank" rel="noopener" href="https://hub.baai.ac.cn/view/24176%EF%BC%8C">https://hub.baai.ac.cn/view/24176，</a> 很有用的ChatGPT相关信息汇总。</p>
</li>
<li><p><strong>张栋：ChatGPT 制胜公式</strong>： <a target="_blank" rel="noopener" href="https://hub.baai.ac.cn/view/24166%EF%BC%8C">https://hub.baai.ac.cn/view/24166，</a> 张提出，ChatGPT = 50% 数据 + 30% 场景 + 10% 算力 + 10% 团队 ，很有趣的观点。</p>
</li>
<li><p><strong>福布斯：下一代大型语言模型</strong>  <a target="_blank" rel="noopener" href="https://www.forbes.com/sites/robtoews/2023/02/07/the-next-generation-of-large-language-models/?sh=1584307d18db%EF%BC%8C">https://www.forbes.com/sites/robtoews/2023/02/07/the-next-generation-of-large-language-models/?sh=1584307d18db，</a> 文中观点合理，主要针对ChatGPT的缺点，提出LLM的升级更新需：1）可以生成自己的训练数据来提高自己的水平；2）可以自我核实事实的模型，这是ChatGPT目前版本不具有的能力；3）大量稀疏的专家模型，类似MoE的稀疏触发；个人觉得还有专门的存储模块。</p>
</li>
<li><p><strong>ChatGPT失败汇总</strong>： <a target="_blank" rel="noopener" href="https://hub.baai.ac.cn/view/24147%EF%BC%8C">https://hub.baai.ac.cn/view/24147，</a> 主要提出1. ChatGPT拥有常识的程度和获得常识的方法不确定；2. ChatGPT在多大程度上记忆与理解他们产生的东西，完全捕捉人类的思想，仍然是未知的；3. ChatGPT有必要进一步改进，提供回答的自信程度。4. 后续研究必须考虑到ChatGPT的道德和社会后果。</p>
</li>
<li><p><strong>数据角度分析，ChatGPT数据集之谜</strong>： <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/9vOc-OyqvzrO_w5LApurbg%EF%BC%8C">https://mp.weixin.qq.com/s/9vOc-OyqvzrO_w5LApurbg，</a> 内文主要从二级和三级来源收集和推测各LLM训练数据集大小和来源，提倡确保数据集的详细信息公开透明、所有人都可访问且易于理解是有用、紧迫和必要的。（在我看来ChatGPT应该主要是利用了对话场景的数据，此类大数据主要是训练基础模型时用）</p>
</li>
<li><p><strong>对话大模型中的事实错误：ChatGPT 的缺陷</strong>  <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/CwYb1uLnzrz7s9jXeqSynw%EF%BC%8C">https://mp.weixin.qq.com/s/CwYb1uLnzrz7s9jXeqSynw，</a> 本文参照综述Survey of hallucination in natural language generation，简述下 NLG 生成“幻觉”文本的成因，接着详细介绍对话任务中的“幻觉”现象，针对对话任务的“幻觉”评估方法和未来研究方向等。</p>
</li>
<li><p><strong>综述｜检测大型语言模型生成文本的方法</strong> <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/FcEscGHEaZpq7deUVZln7g">https://mp.weixin.qq.com/s/FcEscGHEaZpq7deUVZln7g</a> ，本文旨在提供现有大型语言模型生成文本检测技术的概述，并加强对语言生成模型的控制和管理，其中检测方法分为黑盒检测和白盒检测。</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2302.04023%EF%BC%8C">https://arxiv.org/abs/2302.04023，</a> 本文提出了一个使用<strong>公开数据集定量评估交互式LLM</strong>（如ChatGPT）的框架，主要评估了ChatGPT在多任务、多语言和多模态方面 推理、幻觉和交互性的效果，结论是它是一个不可靠的推理器；ChatGPT像其他LLM一样遭受幻觉问题；ChatGPT的交互特性使人能够与底层的LLM协作，以改进其性能。</p>
</li>
<li><p>科技投资人王煜全：<strong>OpenAI给科技行业敲响警钟，中国必须要有自主“大模型”</strong> <a target="_blank" rel="noopener" href="https://hub.baai.ac.cn/view/24009">https://hub.baai.ac.cn/view/24009</a> 此篇采访有很多深入的思考，包括1）生成式AI为何被巨头关注；2）生成式AI商业模式的讨论；3）ChatGPT技术创新性的意义；4）芯片限制后，我国如何发展自己的AI大模型，值得重复阅读和思考。</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/FhtGD8hDxqAUEQDSe-lTTw">https://mp.weixin.qq.com/s/FhtGD8hDxqAUEQDSe-lTTw</a>  此篇为译文，原文观点很敏锐，探讨了<strong>谷歌被ChatGPT颠覆的可能性</strong>，文中提出谷歌在历史上成功的点是缩短了从问题到答案的距离，并找到了一种将其货币化的方法。而ChatGPT可以利用大量的人类知识来提供一个确切的答案，在提炼、生成、凝聚文本方面具有优势，更具多功能和拓展性。</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://hub.baai.ac.cn/view/23921ChatGPT">https://hub.baai.ac.cn/view/23921ChatGPT</a> <strong>ChatGPT背后的经济账</strong>，原文链接： <a target="_blank" rel="noopener" href="https://sunyan.substack.com/p/the-economics-of-large-language-models">https://sunyan.substack.com/p/the-economics-of-large-language-models</a> ，文章从训练成本、云计算（推理）成本、各类成本效率轨迹，多方面剖析了将LLM纳入当前产品和新产品的经济可行性，结论是：训练大语言模型并不便宜，但也没那么烧钱，训练大语言模型需要大量的前期投入，但这些投入会逐年获得回报。</p>
</li>
<li><p><strong>New AI classifier for indicating AI-written text</strong>  <a target="_blank" rel="noopener" href="https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text/">https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text/</a> 在GPTZero和DetectGPT之后，OpenAI也发布官方AI生成文本分类器，模型细节未释出，主体使用训练数据的人工和机器回应做区分，整体效果暂时只有26% AI-generated 文本被识别正确，最后提及人们比较关注的对教育界的影响和措施。</p>
</li>
<li><p><strong>What Makes a Dialog Agent Useful?</strong> <a target="_blank" rel="noopener" href="https://huggingface.co/blog/dialog-agents">https://huggingface.co/blog/dialog-agents</a> 简单介绍了以ChatGPT为首的各ChatBot背后的技术重点，包括人工反馈学习（RLHF）、有监督微调（SFT）、指令学习（IFT）和思维链（CoT）</p>
</li>
</ol>
<img src="/2022/11/08/Paper-info汇总和记录-updating/p8.png" width=65%>

<ol start="27">
<li><p><a target="_blank" rel="noopener" href="https://hub.baai.ac.cn/view/23717">https://hub.baai.ac.cn/view/23717</a> <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2301.07597.pdf">https://arxiv.org/pdf/2301.07597.pdf</a> 提出了首个<strong>「人类-ChatGPT」问答对比语料集</strong>，并开发了首套支持双语的ChatGPT检测器，并且进行了广泛的人工测评、语言学分析、检测实验，结论比较有趣，ChatGPT并非“无懈可击”，和人类相比可以看出差距。</p>
</li>
<li><p>一篇<strong>各模态生成模型SOTA的简单综述</strong>： <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2301.04655.pdf%EF%BC%8C">https://arxiv.org/pdf/2301.04655.pdf，</a> 方便了解目前各模态的生成sota，图比较粗糙，但归类较好：</p>
</li>
</ol>
<img src="/2022/11/08/Paper-info汇总和记录-updating/p9.png" width=65%>

<ol start="29">
<li><p>有启发的一篇文章：<strong>大型语言模型中语言与思想的分离</strong>： <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2301.06627.pdf&quot;%E4%B8%BB%E8%A6%81%E6%8F%90%E5%87%BA%E8%AF%AD%E8%A8%80%E8%83%BD%E5%8A%9B%E5%BA%94%E8%AF%A5%E5%88%86%E4%B8%BAformal">https://arxiv.org/pdf/2301.06627.pdf&quot;主要提出语言能力应该分为formal</a> competence（形式语言技能）和functional competence（认知能力），前者是目前LLM所胜任的任务，而后者需要针对性建立/开发模块/方法，非next word predition任务所能胜任。</p>
</li>
<li><p>产品角度对于LLMs的看法，相关内文见： <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/t0Ml7E-CvlKfdaUMBGKJBg">https://mp.weixin.qq.com/s/t0Ml7E-CvlKfdaUMBGKJBg</a> 主要观点见下图。</p>
</li>
</ol>
<img src="/2022/11/08/Paper-info汇总和记录-updating/p10.png" width=65%>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://ryanran92.github.io/2022/11/08/Paper-info%E6%B1%87%E6%80%BB%E5%92%8C%E8%AE%B0%E5%BD%95-updating/" data-id="clg6do17p00031gpa18xu7awt" data-title="Paper-info 汇总和记录-updating" class="article-share-link">Compartir</a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2022/12/08/ChatGPT%E6%8A%80%E6%9C%AF%E8%A7%A3%E6%9E%84%E4%B8%8E%E6%80%9D%E8%80%83/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Nuevo</strong>
      <div class="article-nav-title">
        
          ChatGPT技术解构与思考
        
      </div>
    </a>
  
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archivos</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/03/">March 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/01/">January 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/12/">December 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/11/">November 2022</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Posts recientes</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/03/16/%E5%85%B3%E4%BA%8EGPT-4%E7%9A%84%E5%8D%81%E9%97%AE%E5%8D%81%E7%AD%94/">关于GPT-4的十问十答</a>
          </li>
        
          <li>
            <a href="/2023/01/11/%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B(LLM)%E7%9A%84%E4%BD%BF%E7%94%A8%E5%92%8C%E6%80%9D%E8%80%83/">大型语言模型(LLM)的使用和思考</a>
          </li>
        
          <li>
            <a href="/2022/12/08/ChatGPT%E6%8A%80%E6%9C%AF%E8%A7%A3%E6%9E%84%E4%B8%8E%E6%80%9D%E8%80%83/">ChatGPT技术解构与思考</a>
          </li>
        
          <li>
            <a href="/2022/11/08/Paper-info%E6%B1%87%E6%80%BB%E5%92%8C%E8%AE%B0%E5%BD%95-updating/">Paper-info 汇总和记录-updating</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 Ryanran<br>
      Construido por <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>