<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Paper/info 汇总和记录(updating) | Ryanran&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="记录  针对多步推理进行小型语言模型的专门化 http:&#x2F;&#x2F;t.km&#x2F;y32dkf http:&#x2F;&#x2F;t.km&#x2F;dl0no2 之前介绍中提到的第四步模型专业化，以专门针对目标任务专门化模型能力，即将大模型能力集中在特定的目标任务上，使用较小模型多步推理来靠近大模型的涌现能力。另外文中提出了包括数据格式、起始模型等选择方法。 Time: 2023-04-06  LLM-Adapters http:&#x2F;&#x2F;t">
<meta property="og:type" content="article">
<meta property="og:title" content="Paper&#x2F;info 汇总和记录(updating)">
<meta property="og:url" content="https://ryanran92.github.io/2022/11/08/Paper-Info%20Summary%EF%BC%88Updating)/index.html">
<meta property="og:site_name" content="Ryanran&#39;s Blog">
<meta property="og:description" content="记录  针对多步推理进行小型语言模型的专门化 http:&#x2F;&#x2F;t.km&#x2F;y32dkf http:&#x2F;&#x2F;t.km&#x2F;dl0no2 之前介绍中提到的第四步模型专业化，以专门针对目标任务专门化模型能力，即将大模型能力集中在特定的目标任务上，使用较小模型多步推理来靠近大模型的涌现能力。另外文中提出了包括数据格式、起始模型等选择方法。 Time: 2023-04-06  LLM-Adapters http:&#x2F;&#x2F;t">
<meta property="og:locale">
<meta property="og:image" content="https://ryanran92.github.io/2023/03/16/Paper-Info%20Summary%EF%BC%88Updating)/p1.png">
<meta property="og:image" content="https://ryanran92.github.io/2023/03/16/Paper-Info%20Summary%EF%BC%88Updating)/p2.png">
<meta property="og:image" content="https://ryanran92.github.io/2023/03/16/Paper-Info%20Summary%EF%BC%88Updating)/p3.png">
<meta property="og:image" content="https://ryanran92.github.io/2023/03/16/Paper-Info%20Summary%EF%BC%88Updating)/p4.png">
<meta property="og:image" content="https://ryanran92.github.io/2023/03/16/Paper-Info%20Summary%EF%BC%88Updating)/p5.png">
<meta property="og:image" content="https://ryanran92.github.io/2023/03/16/Paper-Info%20Summary%EF%BC%88Updating)/p6.png">
<meta property="og:image" content="https://ryanran92.github.io/2023/03/16/Paper-Info%20Summary%EF%BC%88Updating)/p7.png">
<meta property="og:image" content="https://ryanran92.github.io/2023/03/16/Paper-Info%20Summary%EF%BC%88Updating)/p8.png">
<meta property="og:image" content="https://ryanran92.github.io/2023/03/16/Paper-Info%20Summary%EF%BC%88Updating)/p9.png">
<meta property="og:image" content="https://ryanran92.github.io/2023/03/16/Paper-Info%20Summary%EF%BC%88Updating)/p10.png">
<meta property="article:published_time" content="2022-11-07T16:00:00.000Z">
<meta property="article:modified_time" content="2023-04-07T04:59:29.205Z">
<meta property="article:author" content="Ryanran">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ryanran92.github.io/2023/03/16/Paper-Info%20Summary%EF%BC%88Updating)/p1.png">
  
    <link rel="alternate" href="/atom.xml" title="Ryanran's Blog" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Ryanran&#39;s Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://ryanran92.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-Paper-Info Summary（Updating)" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/11/08/Paper-Info%20Summary%EF%BC%88Updating)/" class="article-date">
  <time class="dt-published" datetime="2022-11-07T16:00:00.000Z" itemprop="datePublished">2022-11-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Paper/info 汇总和记录(updating)
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="记录"><a href="#记录" class="headerlink" title="记录"></a>记录</h2><hr>
<ol>
<li><p>针对多步推理进行小型语言模型的专门化 <a target="_blank" rel="noopener" href="http://t.km/y32dkf" title="https://arxiv.org/pdf/2301.12726.pdf">http://t.km/y32dkf</a> <a target="_blank" rel="noopener" href="http://t.km/dl0no2" title="https://hub.baai.ac.cn/view/25238">http://t.km/dl0no2</a> 之前介绍中提到的第四步模型专业化，以专门针对目标任务专门化模型能力，即将大模型能力集中在特定的目标任务上，使用较小模型多步推理来靠近大模型的涌现能力。另外文中提出了包括数据格式、起始模型等选择方法。 Time: 2023-04-06</p>
</li>
<li><p>LLM-Adapters <a target="_blank" rel="noopener" href="http://t.km/gk3osa" title="https://hub.baai.ac.cn/view/25288">http://t.km/gk3osa</a> <a target="_blank" rel="noopener" href="http://t.km/isqcnr" title="https://arxiv.org/pdf/2304.01933.pdf">http://t.km/isqcnr</a> LLM的局部微调（PEFT）方法，将LLaMa和三种LLM-adapter集成（序列Adapter、平行Adapter和LoRA），在较小规模的LLMs（7B）中使用基于适配器的PEFT，在简单的数学推理数据集的零次推理中，产生了与强大的LLMs（175B）相当的性能。   Time: 2023-04-06 </p>
 <img src="/2023/03/16/Paper-Info Summary（Updating)/p1.png" width=65%>
 </li>
<li><p>AI 将改变一切 <a target="_blank" rel="noopener" href="http://t.km/phamzl" title="https://hub.baai.ac.cn/view/25313">http://t.km/phamzl</a> 观点输出文，整篇为翻译文，有些点翻译的较奇怪，提出一系列有趣的观点：</p>
<img src="/2023/03/16/Paper-Info Summary（Updating)/p2.png" width=65%></li>
<li><p>BloombergGPT: A Large Language Model for Finance <a target="_blank" rel="noopener" href="http://t.km/xq14cx" title="https://arxiv.org/pdf/2303.17564.pdf">http://t.km/xq14cx</a> 构建和训练了专门用于金融领域的LLM，开发了拥有500亿参数的语言模型——BloombergGPT，核心是利用五项金融任务在Bloom基础上构建模型，参照Chinchilla和现有数据，决定构建50B模型，效果超出Bloom，但未和ChatGPT比较。</p>
</li>
<li><p>ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks <a target="_blank" rel="noopener" href="http://t.km/epkdf5" title="https://hub.baai.ac.cn/view/25112">http://t.km/epkdf5</a> <a target="_blank" rel="noopener" href="http://t.km/ec0vph" title="https://arxiv.org/abs/2303.15056">http://t.km/ec0vph</a> ChatGPT在包括相关性、立场、主题和两种框架检测任务中的性能超过人工众包工人，在五分之四的任务上，ChatGPT 的零样本准确率高于 MTurk，且成本便宜20倍。</p>
</li>
</ol>
<img src="/2023/03/16/Paper-Info Summary（Updating)/p3.png" width=65%>

<ol start="6">
<li>MEMORIZING TRANSFORMERS <a target="_blank" rel="noopener" href="http://t.km/5ypze5" title="https://arxiv.org/pdf/2203.08913.pdf">http://t.km/5ypze5</a> <a target="_blank" rel="noopener" href="http://t.km/tygm5w" title="https://hub.baai.ac.cn/view/24957">http://t.km/tygm5w</a>，尝试增加“记忆”机制，提出了kNN-augmented attention，通过使用k-nearest-neighbor lookup外部存储器，增加模型可关注的上下文长度。文中包括从头预训练，或在预训练模型基础上增加记忆，但效率未提及，KNN-LM效率是个问题。</li>
</ol>
<img src="/2023/03/16/Paper-Info Summary（Updating)/p4.png" width=65%>


<ol start="7">
<li>为什么现在的LLM都是Decoder-only的架构？ <a target="_blank" rel="noopener" href="http://t.km/blr3br" title="https://mp.weixin.qq.com/s/ZsHX-M9pisUvG9vqfzdzTQ">http://t.km/blr3br</a> 苏剑林提出：LLM 之所以主要都用 Decoder-only 架构，除了训练效率和工程实现上的优势外，在理论上是因为 Encoder 的双向注意力会存在低秩问题，在同等参数量、同等推理成本下，Decoder-only 架构就是最优选择。没太看懂，需要思考。</li>
<li>Lilian Weng新博文：关于提示工程的介绍 <a target="_blank" rel="noopener" href="http://t.km/2ssfg0" title="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/">http://t.km/2ssfg0</a> <a target="_blank" rel="noopener" href="http://t.km/4r1s5f" title="https://hub.baai.ac.cn/view/24941">http://t.km/4r1s5f</a> 关于Prompt Engineering的汇总介绍，包括基础提升工程、指令提示、CoT、自动提示、知识增强（包括外部调用、检索等）等Prompting，详细内容会借鉴并更新在《大型语言模型(LLM)的使用和思考》中。</li>
<li><a target="_blank" rel="noopener" href="http://t.km/11g3iy" title="https://hub.baai.ac.cn/view/24740">http://t.km/11g3iy</a>，提出对于涌现能力的分析和观点，1是涌现能力在物理学、进化生物学等自然科学中同样存在，如物理学中水的固化；2是涌现能力可能和评价方式有关，比如只评价目标没有过程指标时，可能会凸显涌现能力，但其实是渐进式发展的；3是讨论模型是否可以做到更大，结论是受数据所限。</li>
<li>用大语言模型GPT-3直接替代传统搜索引擎：<a target="_blank" rel="noopener" href="http://t.km/4mmlu1" title="https://arxiv.org/abs/2209.10063">http://t.km/4mmlu1</a>，<a target="_blank" rel="noopener" href="http://t.km/0ygusm" title="https://mp.weixin.qq.com/s/JDJ0JdW77NyRaWRFH2Jdkw">http://t.km/0ygusm</a>，核心即先生成再阅读（Generate-then-Read），首先向大型语言模型提示（prompt）生成基于给定问题的上下文文档，然后阅读生成的文档以产生最终答案，和基于检索增强的LM思想一致。</li>
</ol>
<img src="/2023/03/16/Paper-Info Summary（Updating)/p5.png" width=65%>

<ol start="11">
<li>Guiding Large Language Models via Directional Stimulus Prompting <a target="_blank" rel="noopener" href="http://t.km/xsis03" title="https://arxiv.org/pdf/2302.11520.pdf">http://t.km/xsis03</a> 使用policy LM生成一个stimulus（一组关键词），即加了一步中间过程：原文-关键词-输出，关键词这里叫stimulus（刺激），剩余方式和ChatGPT一致，SFT-&gt;RL。</li>
</ol>
<img src="/2023/03/16/Paper-Info Summary（Updating)/p6.png" width=65%>


<ol start="12">
<li>The Capacity for Moral Self-Correction in Large Language Models <a target="_blank" rel="noopener" href="http://t.km/5b4jke" title="https://arxiv.org/pdf/2302.07459.pdf">http://t.km/5b4jke</a> 用“魔法”对抗“魔法”，核心给大模型增加新的指令(IF)，新的思考时间(CoT)来引导大模型减少有害输出，让其自我修正，在实际应用中，也可以增加相关的Prompt，很大概率可以解决，但非根本解决方案。</li>
<li>Why did all of the public reproduction of GPT-3 fail? <a target="_blank" rel="noopener" href="http://t.km/oa1x04" title="https://hub.baai.ac.cn/view/24306">http://t.km/oa1x04</a> <a target="_blank" rel="noopener" href="http://t.km/w5geps" title="https://jingfengyang.github.io/gpt">http://t.km/w5geps</a> ，写的很好很详尽的一篇文字，解释其他大模型无法复现GPT3的原因，同时给出使用LLM的decision tree，非无脑使用，很多时候在确定任务集，使用微调百亿模型即可达到SOTA。</li>
</ol>
<img src="/2023/03/16/Paper-Info Summary（Updating)/p7.png" width=65%>

<ol start="14">
<li>ChatGPT必读论文、博客和API工具（含中文指南）：<a target="_blank" rel="noopener" href="http://t.km/zaz2zo" title="https://hub.baai.ac.cn/view/24176">http://t.km/zaz2zo</a>，很有用的ChatGPT相关信息汇总。</li>
<li>张栋：ChatGPT 制胜公式：<a target="_blank" rel="noopener" href="http://t.km/4g5vls" title="https://hub.baai.ac.cn/view/24166">http://t.km/4g5vls</a>，张提出，ChatGPT = 50% 数据 + 30% 场景 + 10% 算力 + 10% 团队 ，很有趣的观点。</li>
<li>福布斯：下一代大型语言模型 <a target="_blank" rel="noopener" href="http://t.km/yiwu35" title="https://www.forbes.com/sites/robtoews/2023/02/07/the-next-generation-of-large-language-models/?sh=1584307d18db">http://t.km/yiwu35</a>，文中观点合理，主要针对ChatGPT的缺点，提出LLM的升级更新需：1）可以生成自己的训练数据来提高自己的水平；2）可以自我核实事实的模型，这是ChatGPT目前版本不具有的能力；3）大量稀疏的专家模型，类似MoE的稀疏触发；个人觉得还有专门的存储模块。</li>
<li>ChatGPT失败汇总：<a target="_blank" rel="noopener" href="http://t.km/fofhcx" title="https://hub.baai.ac.cn/view/24147">http://t.km/fofhcx</a>，主要提出1. ChatGPT拥有常识的程度和获得常识的方法不确定；2. ChatGPT在多大程度上记忆与理解他们产生的东西，完全捕捉人类的思想，仍然是未知的；3. ChatGPT有必要进一步改进，提供回答的自信程度。4. 后续研究必须考虑到ChatGPT的道德和社会后果。</li>
<li>数据角度分析，ChatGPT数据集之谜：<a target="_blank" rel="noopener" href="http://t.km/051ye1" title="https://mp.weixin.qq.com/s/9vOc-OyqvzrO_w5LApurbg">http://t.km/051ye1</a>，内文主要从二级和三级来源收集和推测各LLM训练数据集大小和来源，提倡确保数据集的详细信息公开透明、所有人都可访问且易于理解是有用、紧迫和必要的。（在我看来ChatGPT应该主要是利用了对话场景的数据，此类大数据主要是训练基础模型时用）</li>
<li>对话大模型中的事实错误：ChatGPT 的缺陷 <a target="_blank" rel="noopener" href="http://t.km/21vfp2" title="https://mp.weixin.qq.com/s/CwYb1uLnzrz7s9jXeqSynw">http://t.km/21vfp2</a>，本文参照综述Survey of hallucination in natural language generation，简述下 NLG 生成“幻觉”文本的成因，接着详细介绍对话任务中的“幻觉”现象，针对对话任务的“幻觉”评估方法和未来研究方向等。</li>
<li>综述｜检测大型语言模型生成文本的方法 <a target="_blank" rel="noopener" href="http://t.km/wiwydw" title="https://mp.weixin.qq.com/s/FcEscGHEaZpq7deUVZln7g">http://t.km/wiwydw</a>，本文旨在提供现有大型语言模型生成文本检测技术的概述，并加强对语言生成模型的控制和管理，其中检测方法分为黑盒检测和白盒检测。</li>
<li><a target="_blank" rel="noopener" href="http://t.km/dna1fj" title="https://arxiv.org/abs/2302.04023">http://t.km/dna1fj</a>，本文提出了一个使用公开数据集定量评估交互式LLM（如ChatGPT）的框架，主要评估了ChatGPT在多任务、多语言和多模态方面 推理、幻觉和交互性的效果，结论是它是一个不可靠的推理器；ChatGPT像其他LLM一样遭受幻觉问题；ChatGPT的交互特性使人能够与底层的LLM协作，以改进其性能。</li>
<li>科技投资人王煜全：OpenAI给科技行业敲响警钟，中国必须要有自主“大模型” <a target="_blank" rel="noopener" href="http://t.km/affcw1" title="https://hub.baai.ac.cn/view/24009">http://t.km/affcw1</a> 此篇采访有很多深入的思考，包括1）生成式AI为何被巨头关注；2）生成式AI商业模式的讨论；3）ChatGPT技术创新性的意义；4）芯片限制后，我国如何发展自己的AI大模型，值得重复阅读和思考。</li>
<li><a target="_blank" rel="noopener" href="http://t.km/1rnpna" title="https://mp.weixin.qq.com/s/FhtGD8hDxqAUEQDSe-lTTw">http://t.km/1rnpna</a> 此篇为译文，原文观点很敏锐，探讨了谷歌被ChatGPT颠覆的可能性，文中提出谷歌在历史上成功的点是缩短了从问题到答案的距离，并找到了一种将其货币化的方法。而ChatGPT可以利用大量的人类知识来提供一个确切的答案，在提炼、生成、凝聚文本方面具有优势，更具多功能和拓展性。</li>
<li><a target="_blank" rel="noopener" href="http://t.km/05jtkd" title="https://hub.baai.ac.cn/view/23921ChatGPT">http://t.km/05jtkd</a>背后的经济账，原文链接：<a target="_blank" rel="noopener" href="http://t.km/bnzit0" title="https://sunyan.substack.com/p/the-economics-of-large-language-models">http://t.km/bnzit0</a>，文章从训练成本、云计算（推理）成本、各类成本效率轨迹，多方面剖析了将LLM纳入当前产品和新产品的经济可行性，结论是：训练大语言模型并不便宜，但也没那么烧钱，训练大语言模型需要大量的前期投入，但这些投入会逐年获得回报。</li>
<li>New AI classifier for indicating AI-written text <a target="_blank" rel="noopener" href="http://t.km/ycrsg0" title="https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text/">http://t.km/ycrsg0</a> 在GPTZero和DetectGPT之后，OpenAI也发布官方AI生成文本分类器，模型细节未释出，主体使用训练数据的人工和机器回应做区分，整体效果暂时只有26% AI-generated 文本被识别正确，最后提及人们比较关注的对教育界的影响和措施。</li>
<li>What Makes a Dialog Agent Useful? <a target="_blank" rel="noopener" href="http://t.km/213kdi" title="https://huggingface.co/blog/dialog-agents">http://t.km/213kdi</a> 简单介绍了以ChatGPT为首的各ChatBot背后的技术重点，包括人工反馈学习（RLHF）、有监督微调（SFT）、指令学习（IFT）和思维链（CoT）</li>
</ol>
<img src="/2023/03/16/Paper-Info Summary（Updating)/p8.png" width=65%>

<ol start="27">
<li><a target="_blank" rel="noopener" href="http://t.km/1gf5jh" title="https://hub.baai.ac.cn/view/23717">http://t.km/1gf5jh</a> <a target="_blank" rel="noopener" href="http://t.km/4tkk1k" title="https://arxiv.org/pdf/2301.07597.pdf">http://t.km/4tkk1k</a> 提出了首个「人类-ChatGPT」问答对比语料集，并开发了首套支持双语的ChatGPT检测器，并且进行了广泛的人工测评、语言学分析、检测实验，结论比较有趣，ChatGPT并非“无懈可击”，和人类相比可以看出差距。</li>
<li>一篇各模态生成模型sota的简单综述：<a target="_blank" rel="noopener" href="http://t.km/oedfos" title="https://arxiv.org/pdf/2301.04655.pdf">http://t.km/oedfos</a>，方便了解目前各模态的生成sota，图比较粗糙，但归类较好：</li>
</ol>
<img src="/2023/03/16/Paper-Info Summary（Updating)/p9.png" width=65%>

<ol start="29">
<li>有启发的一篇文章：大型语言模型中语言与思想的分离：<a target="_blank" rel="noopener" href="http://t.km/svqape" title="https://arxiv.org/pdf/2301.06627.pdf">http://t.km/svqape</a> 主要提出语言能力应该分为formal competence（形式语言技能）和functional competence（认知能力），前者是目前LLM所胜任的任务，而后者需要针对性建立/开发模块/方法，非next word predition任务所能胜任。</li>
<li>产品角度对于LLMs的看法，相关内文见：<a target="_blank" rel="noopener" href="http://t.km/2z54xd" title="https://mp.weixin.qq.com/s/t0Ml7E-CvlKfdaUMBGKJBg">http://t.km/2z54xd</a> 主要观点见下图。</li>
</ol>
<img src="/2023/03/16/Paper-Info Summary（Updating)/p10.png" width=65%>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://ryanran92.github.io/2022/11/08/Paper-Info%20Summary%EF%BC%88Updating)/" data-id="clg62ve5a0000bopagsgy0dyg" data-title="Paper/info 汇总和记录(updating)" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2022/12/08/ChatGPT%E6%8A%80%E6%9C%AF%E8%A7%A3%E6%9E%84%E4%B8%8E%E6%80%9D%E8%80%83/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          ChatGPT技术解构与思考
        
      </div>
    </a>
  
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/03/">March 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/01/">January 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/12/">December 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/11/">November 2022</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/03/16/%E5%85%B3%E4%BA%8EGPT-4%E7%9A%84%E5%8D%81%E9%97%AE%E5%8D%81%E7%AD%94/">关于GPT-4的十问十答</a>
          </li>
        
          <li>
            <a href="/2023/01/01/%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B(LLM)%E7%9A%84%E4%BD%BF%E7%94%A8%E5%92%8C%E6%80%9D%E8%80%83/">大型语言模型(LLM)的使用和思考</a>
          </li>
        
          <li>
            <a href="/2022/12/08/ChatGPT%E6%8A%80%E6%9C%AF%E8%A7%A3%E6%9E%84%E4%B8%8E%E6%80%9D%E8%80%83/">ChatGPT技术解构与思考</a>
          </li>
        
          <li>
            <a href="/2022/11/08/Paper-Info%20Summary%EF%BC%88Updating)/">Paper/info 汇总和记录(updating)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 Ryanran<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>