<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Ryanran&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Ryanran&#39;s Blog">
<meta property="og:url" content="https://ryanran92.github.io/index.html">
<meta property="og:site_name" content="Ryanran&#39;s Blog">
<meta property="og:locale">
<meta property="article:author" content="Ryanran">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Ryanran's Blog" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Ryanran&#39;s Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Suche"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Suche"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://ryanran92.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-关于GPT-4的十问十答" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/03/16/%E5%85%B3%E4%BA%8EGPT-4%E7%9A%84%E5%8D%81%E9%97%AE%E5%8D%81%E7%AD%94/" class="article-date">
  <time class="dt-published" datetime="2023-03-15T16:00:00.000Z" itemprop="datePublished">2023-03-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/03/16/%E5%85%B3%E4%BA%8EGPT-4%E7%9A%84%E5%8D%81%E9%97%AE%E5%8D%81%E7%AD%94/">关于GPT-4的十问十答</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><hr>
<p>&emsp; &emsp;GPT-4作为GPT系列的最新模型，其整体<strong>实现逻辑</strong>、<strong>技术结构</strong>与ChatGPT类似，可以将其看作是拥有<strong>更长上文</strong>、能<strong>更好理解复杂指令</strong>、回答<strong>更可靠</strong>、更<strong>风格化</strong>、更<strong>有创意</strong>的图文版升级ChatGPT，故本文未按照从头开始叙述的逻辑，在之前《ChatGPT技术解构与思考》整体脉络之上，结合OpenAI的Technical Report，选取GPT-4的<strong>关键点</strong>作阐述，整理出GPT4核心的十个问题进行剖析。</p>
<p>&emsp;&emsp; 本文首发在腾讯云开发者公众号 <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/faCMgTd7eV5kC08XqOCW-Q">https://mp.weixin.qq.com/s/faCMgTd7eV5kC08XqOCW-Q</a> 、知乎号 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/614499449">https://zhuanlan.zhihu.com/p/614499449</a> 与腾讯科技 <a target="_blank" rel="noopener" href="https://new.qq.com/rain/a/20230316A08C1Y00">https://new.qq.com/rain/a/20230316A08C1Y00</a> 中，本Blog此文进行实时更新。</p>
<h2 id="Q1：GPT-4是什么？"><a href="#Q1：GPT-4是什么？" class="headerlink" title="Q1：GPT-4是什么？"></a>Q1：GPT-4是什么？</h2><hr>
<p>&emsp; &emsp;GPT-4（Generative Pre-trained Transformer 4）是OpenAI发布的最新GPT系列模型，它是一个<strong>大规模的多模态模型</strong>，其可以接受图像和文本输入，产生文本输出，输出任务依旧是一个<strong>自回归的单词预测任务</strong>，这与外界之前的预期<strong>略微不同</strong>，预期中GPT-4多模态会增加语音、图像、视频、文本多模态输入，输出可能也不局限于文字。</p>
<p>&emsp; &emsp;GPT系列模型的整体情况如下图：</p>
<img src="/2023/03/16/关于GPT-4的十问十答/p1.png" width=65%>


<p>&emsp; &emsp;整体来说，GPT-4的能力已在各种<strong>专业和学术基准上</strong>表现出了人类的水平，包括以大约前10%的成绩通过模拟律师资格考试，而对于生成式的幻觉、安全问题均有<strong>较大的改善</strong>；同时因对于图片模态的强大识别能力扩大了GPT-4的应用范围。</p>
<h2 id="Q2：效果：GPT-4相比ChatGPT和其他GPT模型，效果层面有哪些显著的改进或新增能力？表现在哪些方面？"><a href="#Q2：效果：GPT-4相比ChatGPT和其他GPT模型，效果层面有哪些显著的改进或新增能力？表现在哪些方面？" class="headerlink" title="Q2：效果：GPT-4相比ChatGPT和其他GPT模型，效果层面有哪些显著的改进或新增能力？表现在哪些方面？"></a>Q2：效果：GPT-4相比ChatGPT和其他GPT模型，效果层面有哪些显著的改进或新增能力？表现在哪些方面？</h2><hr>
<p>&emsp; &emsp;GPT-4毫无疑问是目前最强的文本生成模型，<strong>GPT系列模型</strong>整体可以总结为下图：</p>
<img src="/2023/03/16/关于GPT-4的十问十答/p2.png" width=65%>
 
 
<p>&emsp; &emsp;GPT-4的改进具体表现在：</p>
<p>&emsp; &emsp;1） 突破纯文字的模态，<strong>增加了图像模态的输入</strong>，具有强大的图像理解能力。</p>
<p>&emsp; &emsp;让人惊奇的是，GPT-4在4个场景下（4/8）零样本效果超过fine-tuned的SOTA。</p>
<img src="/2023/03/16/关于GPT-4的十问十答/p3.png" width=65%>

<p>&emsp; &emsp;同时它可以解决各类图文混合的理解和生成问题，此处简单举两个例子，一个是根据图表计算格鲁吉亚和西亚的日均肉消耗量：</p>
<img src="/2023/03/16/关于GPT-4的十问十答/p4.png" width=65%>

<p>&emsp; &emsp;一个是解决法语的物理问题：</p>
<img src="/2023/03/16/关于GPT-4的十问十答/p5.png" width=65%>

<p>&emsp; &emsp;可以看到GPT-4在<strong>多语言理解、图文理解能力</strong>上均很强大并已融会贯通。</p>
<p>&emsp; &emsp;2） 支持更长的上下文窗口</p>
<p>&emsp; &emsp;如之前外网泄露图片中，GPT-4存在两个版本，其支持的上下文分别是8K和32K，是ChatGPT上下文长度的2倍和8倍，其成本也分别为ChatGPT的3倍和6倍。</p>
<img src="/2023/03/16/关于GPT-4的十问十答/p6.png" width=65%>

<p>&emsp; &emsp;3） <strong>复杂任务处理能力</strong>大幅提升</p>
<p>&emsp; &emsp;GPT-4在更复杂、更细微的任务处理上，回答更可靠、更有创意，这在多类考试测验中以及与其他LLM的benchmark比较中得到。</p>
<p>&emsp; &emsp;a. GPT-4在不同年龄段不同类别考试中均名列前茅，平均位列人类头部的10%行列；比如律师职业资格考试前10%，生物学奥赛前1%等，下图可以明显看到，两个版本的GPT-4胜出率很高；</p>
<img src="/2023/03/16/关于GPT-4的十问十答/p7.png" width=65%>

<p>&emsp; &emsp;b. MMLU 等benchmark上，碾压其他大模型</p>
<img src="/2023/03/16/关于GPT-4的十问十答/p8.png" width=65%>

<p>&emsp; &emsp;c. 多语言能力强大，特别是小语种能力也很出色</p>
<img src="/2023/03/16/关于GPT-4的十问十答/p9.png" width=65%>


<p>&emsp; &emsp;4） <strong>改善幻觉、安全等局限性</strong>：</p>
<p>&emsp; &emsp;在各类任务上幻觉问题显著减轻，比最新的 GPT-3.5 模型高 40%：</p>
<img src="/2023/03/16/关于GPT-4的十问十答/p10.png" width=65%>

<p>&emsp; &emsp;同样在安全能力的升级上，GPT-4明显超出ChatGPT和GPT3.5。</p>
<img src="/2023/03/16/关于GPT-4的十问十答/p11.png" width=65%>


<p>&emsp; &emsp;5）建立LLM测试标准</p>
<p>&emsp; &emsp;开源OpenAI Evals，创建和运行基准测试的框架，核心思想是<strong>对GPT-4等模型进行评估，并逐个样本检验性能</strong>，此举是可以让大家指出其模型中的缺点，以帮助 OpenAI 进一步改进模型。</p>
<p>&emsp; &emsp;6）预测模型扩展性</p>
<p>&emsp; &emsp;这一点之前涉及比较少，GPT-4在1/1000的计算量上就实现了扩展性的预测，特别在LLM<strong>不适合广泛调参</strong>的情况下，用较小的模型提前预测训练行为和loss，<strong>极大地提升了训练效率，降低了训练成本，增强了LLM训练的可控性</strong>。</p>
<p>&emsp; &emsp;特别对于Inverse Scaling Prize这个任务，此任务提出了模型性能随规模而下降的几个任务，而GPT-4可以通过提前预测模型扩展性，从而在Inverse Scaling Prize上的Hindsight Neglect任务逆转这一趋势。</p>
<img src="/2023/03/16/关于GPT-4的十问十答/p12.png" width=65%>

<p>&emsp; &emsp;7） 重新实现了整个深度学习栈，从头开始设计了一台<strong>超级计算机</strong></p>
<p>&emsp; &emsp;OpenAI和微软合作，在Azure重建了深度学习堆栈，从头设计了一台专用超级计算机；<strong>基础训练设施的改进和定制，使得更大参数量模型的训练成为可能</strong>；</p>
<p>&emsp; &emsp;8） 风格可控</p>
<p>&emsp; &emsp;此处核心是通过“系统”自定Prompt，让模型可以按照规定风格做任务回复；整体思想比较简单，如下图需要GPT-4回复均按照json形式：</p>
<img src="/2023/03/16/关于GPT-4的十问十答/p13.png" width=65%>

<p>&emsp; &emsp;做风格化的聊天极其擅长：</p>
<img src="/2023/03/16/关于GPT-4的十问十答/p14.png" width=65%>

<h2 id="Q3：训练：GPT-4相较于之前的GPT系列模型，在训练方式、模型架构上有哪些创新和优化？"><a href="#Q3：训练：GPT-4相较于之前的GPT系列模型，在训练方式、模型架构上有哪些创新和优化？" class="headerlink" title="Q3：训练：GPT-4相较于之前的GPT系列模型，在训练方式、模型架构上有哪些创新和优化？"></a>Q3：训练：GPT-4相较于之前的GPT系列模型，在训练方式、模型架构上有哪些创新和优化？</h2><hr>
<p>&emsp; &emsp;整体很黑盒，但可以做一些合理的推测；</p>
<p>&emsp; &emsp;首先，<strong>模型参数量</strong>估计约为<strong>10万到100万亿</strong>量级（为作者<strong>个人预估</strong>，也从另一个角度看出OpenAI定制超算的强大），主要根据OpenAI 2020提出的大模型缩放规律：计算预算增加 10 倍，数据集大小应增加约 1.83 倍，模型大小应增加 5.48 倍。按照下图估计，最右处的灰点极有可能为ChatGPT（或其他GPT3.5类千亿模型），图中可以看出GPT-4计算量约为GPT3.5的1000多倍，则模型容量约为548倍左右，1750亿x548≈100万亿；</p>
<img src="/2023/03/16/关于GPT-4的十问十答/p15.png" width=65%>

<p>&emsp; &emsp;其次，GPT-4模型<strong>训练架构</strong>加入了图像模态的输入，应与最近微软发布的 KOSMOS-1类似，即在预训练阶段输入任意顺序的文本和图像，图像经过 Vision Encoder 向量化，文本经过普通transformer向量化，两者组成多模句向量，训练目标仍为next-word generation。</p>
<p>&emsp; &emsp;再者，对于模型<strong>训练数据</strong>内容和数量，文中提及训练数据中额外增加了包含正误数学问题、强弱推理、矛盾一致陈述及各种意识形态的数据，数据量级同样根据OpenAI 2020的缩放率，训练100万亿的模型，数据量是GPT3.5（45TB数据）的190倍。</p>
<p>&emsp; &emsp;最后，GPT-4是从头训练还是在某些基座模型上得来暂时无从得知；可以确定的是，它增加了<strong>后训练过程</strong>，整个过程类似于做Prompt Engineering+RLHF，核心是让模型知道如何在相应场景下合适的回答问题。</p>
<h2 id="Q4：应用：相比ChatGPT，GPT-4有哪些新的应用亮点和场景？"><a href="#Q4：应用：相比ChatGPT，GPT-4有哪些新的应用亮点和场景？" class="headerlink" title="Q4：应用：相比ChatGPT，GPT-4有哪些新的应用亮点和场景？"></a>Q4：应用：相比ChatGPT，GPT-4有哪些新的应用亮点和场景？</h2><hr>
<p>&emsp; &emsp;GPT-4在增强了安全抵御、任务完成度和图片理解能力后，在ChatGPT基础之上有更多亮点和应用场景：</p>
<p>&emsp; &emsp;1）发布视频中根据潦草的手绘制作类似布局类似的网页：</p>
<img src="/2023/03/16/关于GPT-4的十问十答/p16.png" width=65%>

<p>&emsp; &emsp;to：</p>
<img src="/2023/03/16/关于GPT-4的十问十答/p17.png" width=65%>

<p>&emsp; &emsp;2） 加入视觉模态后，可以扩充到盲人应用（Be my eyes）；强大的多语言能力帮助小语种语言的恢复（Iceland language preserve）、安全能力提升后的反欺诈（Stripe）等应用会应运而生：</p>
<img src="/2023/03/16/关于GPT-4的十问十答/p18.png" width=65%>
<img src="/2023/03/16/关于GPT-4的十问十答/p19.png" width=65%>

<p>&emsp; &emsp;3）在AIGC的版图上，建立以GPT-4以及之后更多模态的大模型为基础，形成多模态x多场景的应用网络（图来源：甲子光年）：</p>
<img src="/2023/03/16/关于GPT-4的十问十答/p20.png" width=65%>

<h2 id="Q5：逻辑问题：GPT-4在生成过程中的逻辑性和准确性上有何改进？是否从根本上得到了解决？"><a href="#Q5：逻辑问题：GPT-4在生成过程中的逻辑性和准确性上有何改进？是否从根本上得到了解决？" class="headerlink" title="Q5：逻辑问题：GPT-4在生成过程中的逻辑性和准确性上有何改进？是否从根本上得到了解决？"></a>Q5：逻辑问题：GPT-4在生成过程中的逻辑性和准确性上有何改进？是否从根本上得到了解决？</h2><hr>
<p>&emsp; &emsp;GPT-4在生成逻辑性和准确性上均取得了进展，需要注意的是，GPT-4基础模型在这项任务上只比GPT-3.5略好一点；然而经过RLHF的<strong>后训练后，效果才有了较大的改进</strong>，后训练整个过程类似于<strong>做Prompt Engineering</strong>+RLHF，核心是让模型知道如何在正确的垂直场景下做出合适的回答。</p>
<p>&emsp; &emsp;可以看到，GPT-4相比GPT-3.5和Anthropic优势较明显，但绝对正确率只有60%左右，尚存在较多弊端，并没有从根本上解决这样的问题，也会是后续持续发展的方向。</p>
<img src="/2023/03/16/关于GPT-4的十问十答/p21.png" width=65%>


<h2 id="Q6：安全问题：GPT-4是否从根本上解决了安全问题，OpenAI采用了哪些策略和技术？"><a href="#Q6：安全问题：GPT-4是否从根本上解决了安全问题，OpenAI采用了哪些策略和技术？" class="headerlink" title="Q6：安全问题：GPT-4是否从根本上解决了安全问题，OpenAI采用了哪些策略和技术？"></a>Q6：安全问题：GPT-4是否从根本上解决了安全问题，OpenAI采用了哪些策略和技术？</h2><hr>
<p>&emsp; &emsp;GPT-4在安全问题上<strong>收效显著</strong>，针对安全问题，GPT-4的主要解决思路是利用安全相关的RLHF ，在训练中加入<strong>额外的安全奖励信号</strong>，奖励由 GPT-4 的zero-shot分类器提供，即文中提到的<strong>RBRM</strong>（rule-based reward models, 基于规则的奖励模型）方法，它是一系列零样本的GPT-4 分类器；</p>
<p>&emsp; &emsp;具体来说，这些分类器接受三种输入：Prompt, Policy model 的输出以及可选的对输出的评估（人工编写）。利用这些不同安全等级的 prompt 进行训练：同时对GPT-4在<strong>不安全回复拒绝回答</strong>的行为，以及<strong>在敏感领域做安全回答</strong>两个场景下作奖励，通过强化学习，最后显著改善安全能力，**不安全内容下降82%<strong>；敏感领域安全回答比率上升</strong>29%**；</p>
<p>&emsp; &emsp;和ChatGPT RLHF的方法类似，Alignment（对齐工作）在此处发挥了较大作用，同时未来也会有持续的发力空间，相比单纯累积模型参数量和数据量的“大力出奇迹”方式，其<strong>计算量相对较小</strong>。如下图，在InstructGPT文献中，加入RLHF的1.3B模型，在整体胜出率上，超出了175B的微调模型，节省了100倍的成本；</p>
<img src="/2023/03/16/关于GPT-4的十问十答/p22.png" width=65%>

<h2 id="Q7：冲击：我们如何应对ChatGPT-GPT-4的冲击？对技术人员、对行业影响是怎样的？"><a href="#Q7：冲击：我们如何应对ChatGPT-GPT-4的冲击？对技术人员、对行业影响是怎样的？" class="headerlink" title="Q7：冲击：我们如何应对ChatGPT/GPT-4的冲击？对技术人员、对行业影响是怎样的？"></a>Q7：冲击：我们如何应对ChatGPT/GPT-4的冲击？对技术人员、对行业影响是怎样的？</h2><hr>
<p>&emsp; &emsp;这个问题在ChatGPT出现之后便存在，GPT-4只是加剧了这样的担忧；对技术人员来说，需要在研究命题、下游任务方面做思考，NLP很多单一子任务会随之消失，会引入新的研究命题：</p>
<p>&emsp; &emsp;1）如何精准提出需求；对ChatGPT进行“催眠”，<strong>Prompting Project</strong>；</p>
<p>&emsp; &emsp;2）如何更正错误：<strong>Neural Editing</strong>；</p>
<p>&emsp; &emsp;3）安全<strong>侦测AI生成</strong>，包括整个生成过程中的安全侦测和控制；</p>
<p>&emsp; &emsp;4） 构建<strong>专有化模型</strong>，专用指令和RLHF发掘下游任务潜力；</p>
<p>&emsp; &emsp;5） <strong>Machine unleaning</strong>（学会忘记数据、隐私保护）等</p>
<p>&emsp; &emsp; 对于行业来说，不同层级的公司，需要在不同模块找立足点。初步来看，<strong>初创企业适合入局中间层、数据平台和应用层，大厂适合入局算力、平台和基础层</strong>。</p>
<img src="/2023/03/16/关于GPT-4的十问十答/p23.png" width=65%>
（图来源：甲子光年）

<h2 id="Q8：趋势：从GPT-4可以看出未来LLM的哪些趋势？未来的研发方向和优化策略是什么？"><a href="#Q8：趋势：从GPT-4可以看出未来LLM的哪些趋势？未来的研发方向和优化策略是什么？" class="headerlink" title="Q8：趋势：从GPT-4可以看出未来LLM的哪些趋势？未来的研发方向和优化策略是什么？"></a>Q8：趋势：从GPT-4可以看出未来LLM的哪些趋势？未来的研发方向和优化策略是什么？</h2><hr>
<p>&emsp; &emsp; 1）<strong>闭源趋势</strong>，网友戏称OpenAI已沦为Closed AI；毕竟从GPT1到GPT-4，模型各类细节越来越闭源和黑盒，大模型战场的竞争因素决定了GPT-4类型的第一梯度模型可能会越来越封闭，成为技术门槛；</p>
<p>&emsp; &emsp; 2）**”羊驼”模式<strong>。之所以叫羊驼模式，来源于Meta的Alpaca，其</strong>核心<strong>是：中小模型+大模型生产指令数据的“LLaMA 7B + text-davinci-003”模式，中小参数的模型在成本上，是</strong>更靠近实际落地的方式**，要知道llama.cpp可以在Pixel 6手机上运行；通过该模式精调过的Alpaca，效果接近普通GPT3.5；</p>
<p>&emsp; &emsp; 3）<strong>更多模态、更多形态</strong>结合ChatGPT类模型：包括Kosmos-1和具身智能PaLM-E，同时从听、说、看、触等全方位结合，形成类似真正智能体的概念；</p>
<p>&emsp; &emsp; 4）<strong>模型加速和降低成本</strong>会是持续关注的方向，包括从训练、推理等多层面考量：</p>
<img src="/2023/03/16/关于GPT-4的十问十答/p24.png" width=65%>

<p>&emsp; &emsp; 5）<strong>能力预测</strong>是很重要的方向；即用小模型来预测广泛大模型的能力，极大减少试错成本，提升训练效率；</p>
<p>&emsp; &emsp; 6） <strong>开源评测框架</strong>对于LLM的评测具有重大意义，可以快速发现改进方向。</p>
<h2 id="Q9：其他：GPT-4论文（technical-report）中，还有哪些值得关注的点？"><a href="#Q9：其他：GPT-4论文（technical-report）中，还有哪些值得关注的点？" class="headerlink" title="Q9：其他：GPT-4论文（technical report）中，还有哪些值得关注的点？"></a>Q9：其他：GPT-4论文（technical report）中，还有哪些值得关注的点？</h2><hr>
<p>&emsp; &emsp; 有一些点比较有趣且可以引发我们的联想，这里提出两点：</p>
<p>&emsp; &emsp; 1）GPT-4出现了“<strong>寻求权力</strong>”的倾向，并警告这一特征的风险</p>
<p>&emsp; &emsp; 文中提到，Novel capabilities often emerge in more powerful models.Some that are particularly concerning are the ability to create and act on long-term plans,to accrue power and resources (“powerseeking”), and to exhibit behavior that is increasingly “agentic”. 即GPT-4开始拥有一些新的能力，包括创建长期计划并采取行动的能力，积累权力和资源（“寻求权力”），以及表现出越来越“代理”的行为，例如，完成可能没有具体规定的、在训练中没有出现的目标；专注于实现具体的、可量化的目标；以及进行长期规划。而此类行为具有突发性。</p>
<p>&emsp; &emsp; 某种程度上，RLHF的模型本身在寻求奖励最优，所以在某些问题上寻求权力可能会是最优的一项选择。</p>
<p>&emsp; &emsp; 2）赋予了GPT-4<strong>自我编码、复制和执行的能力</strong>，甚至启动资金</p>
<p>&emsp; &emsp;在测试GPT-4的过程中，OpenAI引入的外部专家团队ARC(Alignment Research Center)作为“红方”。</p>
<p>&emsp; &emsp; ARC会给GPT-4这样一个操作：允许GPT-4执行代码，进行链式推理，并可以用少量的钱和一个带有语言模型API的账户，用是否能够赚更多的钱来增加其的稳健性，GPT-4已经可以开始自己赚钱了。</p>
<h2 id="Q10：AGI：GPT-4是通往AGI的唯一道路吗？"><a href="#Q10：AGI：GPT-4是通往AGI的唯一道路吗？" class="headerlink" title="Q10：AGI：GPT-4是通往AGI的唯一道路吗？"></a>Q10：AGI：GPT-4是通往AGI的唯一道路吗？</h2><hr>
<p>&emsp; &emsp; 总的来说，ChatGPT/GPT-4这样的模型，是现在距离AGI最近的一条路，但因为其本质为一个<strong>概率预测模型</strong>，没有真正的<strong>逻辑处理</strong>模块，也没有<strong>记忆存储</strong>模块，属于一个不太稳定的系统；另外，它使用外界工具的能力也<strong>尚显初级</strong>，一个真正的AGI一定会像人一样，可以快速学会工具的使用。</p>
<p>&emsp; &emsp; 但GPT大模型的不断进化，让人类看到了触碰到AGI的希望之光。</p>
<h2 id=""><a href="#" class="headerlink" title=""></a></h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><hr>
<ol>
<li>GPT-4 <a target="_blank" rel="noopener" href="https://openai.com/research/gpt-4">https://openai.com/research/gpt-4</a> </li>
<li>GPT-4 is OpenAI’s most advanced system, producing safer and more useful responses <a target="_blank" rel="noopener" href="https://openai.com/product/gpt-4">https://openai.com/product/gpt-4</a> </li>
<li>GPT-4 Technical Report <a target="_blank" rel="noopener" href="https://cdn.openai.com/papers/gpt-4.pdf">https://cdn.openai.com/papers/gpt-4.pdf</a> </li>
<li>GPT-4震撼发布-机器之心 <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/kA7FBZsT6SIvwIkRwFS-xw">https://mp.weixin.qq.com/s/kA7FBZsT6SIvwIkRwFS-xw</a> </li>
<li>In AI, is bigger always better? <a target="_blank" rel="noopener" href="https://www.nature.com/articles/d41586-023-00641-w">https://www.nature.com/articles/d41586-023-00641-w</a>  <a target="_blank" rel="noopener" href="https://hub.baai.ac.cn/view/24755">Nature | 在AI领域，模型越大意味着越好吗？ - 智源社区</a></li>
<li>Scaling Laws for Neural Language Models <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2001.08361.pdf">https://arxiv.org/pdf/2001.08361.pdf</a></li>
<li> LLaMA: Open and Efficient Foundation Language Models <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2302.13971.pdf">https://arxiv.org/pdf/2302.13971.pdf</a></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://ryanran92.github.io/2023/03/16/%E5%85%B3%E4%BA%8EGPT-4%E7%9A%84%E5%8D%81%E9%97%AE%E5%8D%81%E7%AD%94/" data-id="clg619vqw0000s4pah3occokq" data-title="关于GPT-4的十问十答" class="article-share-link">Teilen</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-大型语言模型(LLM)的使用和思考" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/01/01/%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B(LLM)%E7%9A%84%E4%BD%BF%E7%94%A8%E5%92%8C%E6%80%9D%E8%80%83/" class="article-date">
  <time class="dt-published" datetime="2022-12-31T16:00:00.000Z" itemprop="datePublished">2023-01-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/01/01/%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B(LLM)%E7%9A%84%E4%BD%BF%E7%94%A8%E5%92%8C%E6%80%9D%E8%80%83/">大型语言模型(LLM)的使用和思考</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="1-导论"><a href="#1-导论" class="headerlink" title="1.  导论"></a>1.  导论</h1><p>&emsp;  &emsp;  ChatGPT的出现和“爆火”，让大家见识了大型语言模型(Large Language Models，下文均简称LLM)的威力，尤其是千亿模型已经远超BERT时代的认知，甚至看到了AGI（Artificial General Intelligence，通用人工智能）的可能性。但随之而来的机遇和挑战并存。除了遥遥领先的OpenAI，业界大部分公司或多或少被拉到了追赶ChatGPT的同一起跑线上。 </p>
<p>&emsp;&emsp;  本文延续上篇文章（<a target="_blank" rel="noopener" href="https://km.woa.com/articles/show/567184">ChatGPT技术解构与思考</a>）末尾对于ChatGPT如何使用的话题，基于最近阅读的一些材料和学习，结合从组内做过的实验中得到的认知，拓展到更广范围的LLM，集中阐述如何使用、适配和挖掘LLM的能力，并给出相关思考供大家讨论。</p>
<h1 id="2-综述"><a href="#2-综述" class="headerlink" title="2.     综述"></a>2.     综述</h1><p>&emsp;&emsp;  通过<strong>海量数据</strong>训练得到的<strong>超大参数</strong>模型蕴含了<strong>海量知识</strong>，以GPT3（175B）开端，再到LaMDA（137B）、Gopher（280B）、FLAN-T5（540B）等，业界对于LLM的探索和应用，以及如何挖掘其学到的知识，引导它们适配不同子任务达到最先进结果（state-of-the-art result，sota），一直是近两年一项极具价值及热门的工作。</p>
<p>&emsp;&emsp;  对于LLM的探索，从起初探索<strong>贴近预训练任务</strong>的方式构造下游任务，包括各类Prompt Engineering方式，减少微调数据量；再到用非梯度更新的方式，使大模型<strong>无需微调</strong>情况下，拥有小样本、零样本解决问题的能力，包括上下文学习（In-context learning）、上下文学习的矫正（Calibration）等；利用LLM解决更难的数理推理问题，通过一系列逻辑链(CoT，chain of thought )，深入挖掘大模型的知识和推理能力；进一步，更加看重行动驱动（Action-driven）、意图驱动与大模型的结合，使大模型从意图出发对齐人类需求；以上探索，力求在数量繁多的自然语言任务中达到初步的“质变”效果，尝试通向真正的AGI。</p>
<p>&emsp;&emsp;  在讨论如何使用LLM之前，首先需要明确我们<strong>为何要发展</strong>这样的技术，我们的机会在哪？进而才能讨论我们如何<strong>发展和适应</strong>这些技术？最后我们需要思考LLM这项技术会给我们带来什么，<strong>机遇和挑战</strong>是什么？</p>
<h1 id="3-我们为何要发展这样的技术？我们还有机会么？"><a href="#3-我们为何要发展这样的技术？我们还有机会么？" class="headerlink" title="3.     我们为何要发展这样的技术？我们还有机会么？"></a>3.     我们为何要发展这样的技术？我们还有机会么？</h1><p>&emsp;&emsp;  基于大模型本身蕴含的海量知识，大型语言模型有机会使得<strong>以文本为主</strong>的业务得到“质变“的提升；再次以最近过于火热的ChatGPT为例，其基于GPT3.5模型，其超强的对话能力、意图理解以及对齐能力，对类似的ChatBot和文本生成等任务具有<strong>颠覆性</strong>；而这样的“Emergent abilities”，无论作为流量入口，或者模型升级有效赋能到各项业务，使业务在<strong>智能化</strong>和<strong>“懂”用户</strong>层面上得到“跃升”，是非常有前景和激动人心的。</p>
<p>&emsp;&emsp;  那么如果我们要做自己的ChatGPT/LLM，还有多远的距离？如果用时间来衡量大约是两年的时间，计算的起点是GPT3的出现，基于的假设是我们现有可用的最强模型Bloom/OPT-IML已达到GPT3能力（数据集指标可以对齐，但整体感受层面尚有差距）；同时，这两年多时间包含着数位高级研究者智力、数百台机器数千张GPU的长期探索。通过参考文献1中提到下面表1看的更清楚，GPT3到现有的ChatGPT，大致有5~6次里程碑的演化。</p>
<table>
<thead>
<tr>
<th align="left">能力</th>
<th align="left">OpenAI模型</th>
<th align="left">训练方法</th>
<th align="left">OpenAI API</th>
<th align="left">OpenAI论文</th>
<th align="left">近似的开源模型</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>GPT3系列</strong></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">语言生成<br>+ 世界知识 <br>+ 上下文学习</td>
<td align="left">GPT-3初始版本<br><strong>大部分的能力已经存在于模型中，尽管表面上看起来很弱。</strong></td>
<td align="left">语言建模</td>
<td align="left">Davinci</td>
<td align="left"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.14165">GPT3论文</a></td>
<td align="left"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.01068">Meta OPT</a></td>
</tr>
<tr>
<td align="left">+ 遵循人类的指令<br><strong>+ 泛化到没有见过的任务</strong></td>
<td align="left">Instruct-GPT初始版本</td>
<td align="left">指令微调</td>
<td align="left">Davinci-Instruct-Beta</td>
<td align="left"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.02155">Instruct-GPT论文</a></td>
<td align="left"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2110.08207">T0论文</a> <br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.11416">Google FLAN论文</a> <br></td>
</tr>
<tr>
<td align="left">+ 代码理解<br>+ 代码生成</td>
<td align="left">Codex初始版本</td>
<td align="left">在代码上进行训练</td>
<td align="left">Code-Cushman-001</td>
<td align="left"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.03374">Codex论文</a></td>
<td align="left"><a target="_blank" rel="noopener" href="https://github.com/salesforce/CodeGen">Salesforce CodeGen</a></td>
</tr>
<tr>
<td align="left"><strong>GPT3.5系列</strong></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">++ 代码理解<br>++ 代码生成<br>++ <strong>复杂推理 / 思维链</strong><br>+ 长距离的依赖  (很可能)</td>
<td align="left">现在的Codex<br><strong>GPT3.5系列中最强大的模型</strong></td>
<td align="left">在代码+文本上进行训练<br>在指令上进行微调</td>
<td align="left">Code-Davinci-002 <br>(目前免费的版本 = 2022年12月)</td>
<td align="left"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.03374">Codex论文</a></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">++ 遵循人类指令<br>- 上下文学习<br>- 推理能力<br>++ 零样本生成</td>
<td align="left">有监督的Instruct-GPT <br><strong>通过牺牲上下文学习换取零样本生成的能力</strong></td>
<td align="left">监督学习版的指令微调**</td>
<td align="left">Text-Davinci-002</td>
<td align="left"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.02155">InsructGPT论文</a> 有监督部分</td>
<td align="left"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2110.08207">T0论文</a> <br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.11416">Google FLAN论文</a> <br></td>
</tr>
<tr>
<td align="left">+ 遵循人类价值观<br>+ 包含更多细节的生成<br>+ 上下文学习<br>+ 零样本生成</td>
<td align="left">经过RLHF训练的Instruct-GPT<br>**和002模型相比，和人类更加对齐，并且更少的性能损失</td>
<td align="left">强化学习版的指令微调**</td>
<td align="left">Text-Davinci-003</td>
<td align="left"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.02155">InsructGPT论文</a>  从人类反馈中学习</td>
<td align="left"><a target="_blank" rel="noopener" href="https://www.deepmind.com/blog/building-safer-dialogue-agents">Deepmind Sparrow</a> <br><a target="_blank" rel="noopener" href="https://github.com/allenai/RL4LMs">AI2 RL4LMs</a></td>
</tr>
<tr>
<td align="left">++ 遵循人类价值观<br>++ 包含更多细节的生成<br>++ <strong>拒绝知识范围外的问题</strong> <br>++ 建模对话历史的能力<br>– 上下文学习</td>
<td align="left">ChatGPT<br>** 通过牺牲上下文学习的能力换取建模对话历史的能力**</td>
<td align="left">使用对话数据进行强化学习指令微调**</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"><a target="_blank" rel="noopener" href="https://www.deepmind.com/blog/building-safer-dialogue-agents">Deepmind Sparrow</a> <br><a target="_blank" rel="noopener" href="https://github.com/allenai/RL4LMs">AI2 RL4LMs</a></td>
</tr>
<tr>
<td align="left"><br></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody></table>
<center>表1  ChatGPT技术演进</center>


<p>&emsp;&emsp;  那么我们还有机会么？答案<strong>当然是有</strong>，并可以在不同组织层面以不同身位入场。LLM的引入会使行业内公司划分出不同层级（此处很同意谢剑的观点，主要化用其结论，详细内容参见参考文献7)</p>
<p>&emsp;&emsp;  1） Level1：LLM基础设施公司；类比为一个拥有比较强通用能力的人；Level1的公司会比较少，可能只有1-2家（比如 OpenAI、Google）</p>
<p>&emsp;&emsp;  2） Level2：基于LLM<strong>结合场景</strong>进行<strong>商业化应用</strong>的公司(以应用为主，没有全体微调) ；类比通用能力的人去一些场景打工挣钱；Level2的公司侧重于基于LLM做出较多应用，包括从头创新做应用、已有的业务场景中升级功能。</p>
<p>&emsp;&emsp;   3） Level3：基于LLM+领域场景数据，微调形成具备更强领域能力和一定门槛的产品，通过商业化和数据积累，持续形成<strong>业务数据-模型闭环</strong>；类比一个领域专家。Level3的公司有很强的专业性和数据积累，比如类似专项面向写作的Jasper.AI等。</p>
<p>&emsp;&emsp;  我们在LLM层级以如何的身位入场，是个值得大家思考和讨论的问题。而对于不同Level对于个人研究者的发展层级，也可以从以上3个层级对应角色和工作，具体也可参照以上链接。</p>
<h1 id="4-我们如何适应LLM技术：使用方式"><a href="#4-我们如何适应LLM技术：使用方式" class="headerlink" title="4.   我们如何适应LLM技术：使用方式"></a>4.   我们如何适应LLM技术：使用方式</h1><h3 id="4-1-预训练"><a href="#4-1-预训练" class="headerlink" title="4.1 预训练"></a>4.1 预训练</h3><p>&emsp;&emsp;  发展LLM技术最直接的方式一定是从头预训练（pretrain from scratch），它可以按照不同需求量身定制做预训练，这样的方式对于训练千亿级别的模型，核心问题是老生常谈的成本问题。2年半前的GPT-3预计训练成本接近 500 万美元，此条路径适合专业的、大组织形式内的整体战略和输出，对于小规模研究组/项目组并不合适。</p>
<h3 id="4-2-微调"><a href="#4-2-微调" class="headerlink" title="4.2 微调"></a>4.2 微调</h3><p>&emsp;&emsp;  基于已有的LLM，适配到下游任务最直接的方式是微调，也是ChatGPT第一阶段训练方式。对于较大组织，在LLM基础上做二阶段预训练/微调是一条可行之道；但对于业务方向来说，直接微调千亿模型的消耗成本也过高，需要探索<strong>提升训练效率、减少训练参数量</strong>。而基于以上诉求，业界逐渐产生了多种局部微调/无需微调使用LLM的方法。</p>
<h3 id="4-3-基于Prompt的微调（Prompt-based-Finetune）"><a href="#4-3-基于Prompt的微调（Prompt-based-Finetune）" class="headerlink" title="4.3 基于Prompt的微调（Prompt-based Finetune）"></a>4.3 基于Prompt的微调（Prompt-based Finetune）</h3><p>&emsp;&emsp;   第一类是Prompt-based方法，它是较早应用于普通预训练模型的方法，在文本理解(NLU)层级使用较多，其核心思想是增强预训练任务和下游<strong>任务的一致性</strong>，保证这样的一致性，可以让预训练模型在样本较少的情境下效果超越微调；同时当模型效果接近全数据微调时，所需标注数据量也大大减少，尤其对一些较简单的分类任务效果明显。</p>
<p>&emsp;&emsp;  Prompt-based 以PET和LM-BFF为代表；</p>
<p>&emsp;&emsp;   1）PET（Pattern-Exploiting Training）</p>
<p>&emsp;&emsp;  PET方法示意图见图1，即将标签模板化，预测[MASK]位置label的概率。</p>
<img src="/2023/01/01/大型语言模型(LLM)的使用和思考/p1.png" width=65%>


<br>
<center>图1 PET Prompt示意图</center>
<br>
&emsp;&emsp;  2）LM-BFF（better few-shot fine-tuning of language models）
LM-BFF聚焦于如何得到更好的Prompt，其探索了自动Prompt的选择和构造，以表2为例，在NLI任务中不同的Prompt对于下游任务的准确率影响较大。

<img src="/2023/01/01/大型语言模型(LLM)的使用和思考/p2.png" width=65%>
<br>
<center>图2  不同Prompt形式的效果</center>

<p>&emsp;&emsp;  LM-BFF在不同任务的小样本学习效果超出等量数据下的微调，但整体结果不够稳定，方差较大，对人工模板的依赖也较大。</p>
<p>&emsp;&emsp;  之后很多的工作聚焦于如何设计得到更好的Prompt，降低这样的不稳定性，包括对于标签、模板和Prompt整体的搜寻、设计；同时业界也通过更多办法包括挖掘（Mining）、重写(Paraphrasing)、生成(Generation)等方式，来提升Prompt的多样性和稳定性，提升基于Prompt微调的效果；还有其余工作不局限在离散的自然语言的模板，比如P-tuning方法。</p>
<p>&emsp;&emsp;  我们利用Prompt-based方法主要聚焦在分类问题上，分类问题情境一般较简单，使用PET标注量只需原先的1/10（500 vs 5000）。</p>
<h3 id="4-4-参数有效的Prompt调整-Parameter-efficient-Prompt-Tuning"><a href="#4-4-参数有效的Prompt调整-Parameter-efficient-Prompt-Tuning" class="headerlink" title="4.4 参数有效的Prompt调整(Parameter-efficient Prompt Tuning)"></a>4.4 参数有效的Prompt调整(Parameter-efficient Prompt Tuning)</h3><p>&emsp;&emsp;  所谓Parameter-efficient（参数有效），即无需针对每个任务微调一个<strong>全参数</strong>的模型，只需调整LLM的部分参数，下游任务即可达到较好效果，该方法极大降低训练/存储成本。</p>
<p>&emsp;&emsp;   Parameter-efficient Tuning核心的关键是如何选择有效微调的参数？选择哪部分参数微调？这部分参数占比多少？</p>
<p>&emsp;&emsp;  1）Adapter Finetune</p>
<p>&emsp;&emsp;  此方法在Transformers中添加 Adapter Layers，微调过程中只调整此部分参数。Adapter为图3中，在FFN和Norm layer层之间添加的”Adapter”部分，其构成是两层FFN，只需微调3.6% Adapter的参数，在下游任务效果接近全量微调。</p>
<img src="/2023/01/01/大型语言模型(LLM)的使用和思考/p3.png" width=65%>
<center>图3 Adapter结构示意图</center>

<p>&emsp;&emsp;   2） Prefix-tuning</p>
<p>&emsp;&emsp;  Prefix-tuning是第一部分Prompt-based微调的发展和延伸，核心思想是选取与子任务强相关的Prompt参数连续化微调，同时<strong>固定其他部分参数</strong>；该方法优点是无需人工设计模板，可以参数化自动学习模板统一优化；缺点也很明显，相比文字模板解释性较低。</p>
<img src="/2023/01/01/大型语言模型(LLM)的使用和思考/p4.png" width=65%>

<center>图4  Prefix-tuning与Fine-tune区别</center>
<br>
&emsp;&emsp;   Prefix-tuning的离散模板添加在**每层hidden state**前，与其他同为离散化模板方法的P-tuning不同，后者只需维持输入前缀的离散化，无需隐层状态中维持。Prefix-tuning所需调整参数比率为0.1%，同时在table2text、翻译、摘要3个生成任务上，效果超出其他小样本调整方式，甚至效果基本接近整体微调。
<br>
<br>
<img src="/2023/01/01/大型语言模型(LLM)的使用和思考/p5.png" width=65%>
<br>
<center>图5  Table-to-text任务Prefix对照实验结果</center>
<br>
&emsp;&emsp;  我们利用Prefix-Tuning，主要在小样本的结构化生成任务中有过尝试，但生成效果一般，没有得到论文中的结果，且相比微调的稳定性稍差；可能是因为迁移到中文的影响，抑或没有选取到合适的参数；但其选取部分hidden-state的微调思想很有借鉴意义，也说明在相对垂直的一些任务上，无需很大参数量，局部微调就可达成目标，LLM的参数在这些任务上是**过分冗余**的。

<p>&emsp;&emsp;   3)  Prompt tuning </p>
<p>&emsp;&emsp;  Prompt tuning 可以看做Prefix-tuning的特殊形式，Google提出的Prompt tuning是做了全面系统的多实验参数的精调，来梳理清晰Prompt tuning的影响因素。此处的结论很有意思：</p>
<p>&emsp;&emsp;  ● Prompt长度：当模型参数&lt;10亿时，整体倾向是：随着长度变长，模型效果持续上升，在长度为20时即达到峰值；但当模型规模超过10亿时，长度影响差距缩小，特别长度为1时效果也尚可。</p>
<img src="/2023/01/01/大型语言模型(LLM)的使用和思考/p6.png" width=65%>
<br>
<center>图6  Prompt长度在SuperGLUE的结果分布</center>
<br>
&emsp;&emsp;  ● Prompt初始化方法：当模型参数<10亿时，初始化时，使用分类标签的embeddings与随机选取词表中词的embeddings作初始化，效果相比完全随机初始化好很多，且后者极不稳定；但当模型参数大于10亿后，选取何种方法初始化**不再重要**。
<br>
<br><img src="/2023/01/01/大型语言模型(LLM)的使用和思考/p7.png" width=65%>
<br>
<center>图7  Prompt初始化方法在SuperGLUE的结果分布</center>

<p>&emsp;&emsp;   ● Prompt预训练方法：LM Adapation效果稳定，随着模型参数上升稳步增长，另外两种方式极其不稳定；但神奇的是，当模型同样达到10亿量级后，模型效果均<strong>趋向良好</strong>。</p>
<img src="/2023/01/01/大型语言模型(LLM)的使用和思考/p8.png" width=65%>
<br>
<center>图8  预训练方法在SuperGLUE的结果分布</center>
<br>
&emsp;&emsp;  ● 训练步数：在普通量级参数量下，步数越多效果越好，但当模型量级达到10亿后，模型零样本/小样本能力凸显，无需过多步数训练。
<br>
<br>
<img src="/2023/01/01/大型语言模型(LLM)的使用和思考/p9.png" width=65%>
<br>
<center>图9  训练步数在SuperGLUE的结果分布</center>
<br>
&emsp;&emsp;  以上实验可以看出，在Prompt tuning上模型**规模的力量**（the power of scale）。当模型参数达到一定量级后，无效的模型设计对结果影响不大；模型开始“返璞归真”，暴力美学尽显，这与对话模型PLATO异曲同工：PLATO-1和PLATO-2一顿操作设计猛如虎，但PLATO-XL只用最简单的Transformers结构，只在数据量和参数量大幅增加后即呈碾压之势。
相关工作还有Prompt tuning的升级工作，包括模板融合等。此部分我们没有做过多的参数实验，只在相同模型（1亿内量级）下做过一些Prompt的修改，但影响整体不大。

<h3 id="4-5-上下文学习（In-context-Learning）"><a href="#4-5-上下文学习（In-context-Learning）" class="headerlink" title="4.5 上下文学习（In-context Learning）"></a>4.5 上下文学习（In-context Learning）</h3><p>&emsp;&emsp;  上下文学习主要在GPT3发布后，开始引发业界关注并逐渐流行；它无需在大模型上进行梯度更新，无需任何形式参数的微调，只需少量下游任务的任务描述和输入输出示例（甚至完全零样本学习），即可在下游任务上取得不错的效果；当选取最大的1750亿参数的模型时，部分任务上甚至超过出SOTA，如在LAMBADA（开放式完形填空任务任务）上提升了18%；图10很好的说明了In-context Learning与传统微调的区别。</p>
<img src="/2023/01/01/大型语言模型(LLM)的使用和思考/p10.png" width=65%>
<br>
<center>图10  In-context learning和微调区别</center>

<p>&emsp;&emsp;  1）基础In-context learning</p>
<p>&ensp; &ensp; In-context learning主要由以下四个部分组成：Distribution of inputs、Label space、Format、Input-label mapping，图11清晰的阐述可该组成结构。<br><br><br><img src="/2023/01/01/大型语言模型(LLM)的使用和思考/p11.png" width=65%><br><br></p>
<center>图11  In-context learning结构</center>

<p>&emsp;&emsp;  显然In-context learning的优化就从以上四个方面着手设计和探索。</p>
<p>&emsp;&emsp;   ● Distribution of inputs即为原始文本输入</p>
<p>&emsp;&emsp;  对于输入样本的<strong>选择和顺序</strong>均会影响In-context learning的效果。前者同小样本学习的样本选择类似，需要选择有代表性、分布均匀的样本，拿分类问题举例，尽可能提供包含不同类别不同的分布；后者是指不同样本的顺序会影响效果，在参考文献2中有阐述，在SST-2（单句情感分类任务）判断任务中，不同排列顺序的输入会导致较大浮动范围的指标。 </p>
<img src="/2023/01/01/大型语言模型(LLM)的使用和思考/p12.png" width=65%>
<br>
<center>图12  SST-2不同样本顺序结果浮动</center>
<br>
&emsp;&emsp;   ● Lable space即为标签的定义和分布；
Label space相关的课题主要是研究label空间的重要性，但整体的结论对模型训练影响不大，提供正确label词即可。

<p>&emsp;&emsp;  ● Format即Demonstration的格式<br>对于Format的研究，意图寻求最好的Format，比如面向复杂推理问题的CoT（逻辑链推到，下文会介绍）便是一种对于合适Format的探索；Format在In-context learning中很重要，格式是否相同、格式是否统一，均会对模型表现有加大的影响。</p>
<p>&emsp;&emsp;  ● Input-label mapping可以理解为得到标签的计算方式</p>
<p>&emsp;&emsp;  传统计算方式为计算下一个词的条件概率，也可计算整句PPL概率；</p>
<p>&emsp;&emsp;   针对In-context Learning一种较好的方式是<strong>Channel函数</strong>，它主要面向不平衡标签数据，通过反向计算，即给定标签计算输入概率，选择最大概率来得到最后的标签。它主要防止当标签不平衡时，引起label预测不平衡。</p>
<p>&emsp;&emsp;  在我们的实际工作基于百亿/千亿模型的数据扩增中，In-context learning起到了主要作用；在人工精标少量分布均匀的种子数据后，通过设计In-context learning的示例，可以快速扩增训练数据，进而供下游任务使用。</p>
<p>&emsp;&emsp;  2）In-context learning的校准（Calibration）</p>
<p>&emsp;&emsp;  在上文提到，不同样本Demonstrations的顺序效果浮动超出可控范围；实际场景中，除了上文的影响，存在<strong>多类bias</strong>会影响语言模型本身的倾向，以下三种bias影响模型结果：</p>
<p>&emsp;&emsp;  ● <strong>标签分布bias</strong>；当Demonstrations中标签不平衡时，语言模型倾向于预测<strong>占多数</strong>的标签；</p>
<p>&emsp;&emsp;  ● <strong>词频bias</strong>；当存在两类标签A和B，但B为一个低频词时，此时模型大概率倾向于预测<strong>词频高</strong>的A标签；</p>
<p>&emsp;&emsp;   ● <strong>就近bias</strong>；即模型会易受最新见到的标签影响，如样本标签顺序为PNNPP，预测<strong>就近的</strong>P的概率会较大。</p>
<p>&emsp;&emsp;   以上客观存在的bias，需要在In-context learning中做校准（Calibration），主体方法是得到bias的概率分布后，如P(Pos)=0.65，P(Neg)=0.35，使用校准矩阵W校准，W为概率倒数形成的矩阵，其他bias的矫正思想也类似。</p>
<p><img src="/2023/01/01/大型语言模型(LLM)的使用和思考/p13-1.png" width=65%><img src="/2023/01/01/大型语言模型(LLM)的使用和思考/p13-2.png" width=35%><br><br></p>
<center>图13  校准概率函数与校准矩阵</center>
<br>
&emsp;&emsp;  In-context learning从出现到兴起，其有效性的由来完全黑盒。也有工作开始尝试进行解释，此篇文献尝试从贝叶斯推理的角度进行解释：An Explanation of In-context Learning as Implicit Bayesian Inference ，有兴趣的同学可以深入研究。

<h3 id="4-6-推理"><a href="#4-6-推理" class="headerlink" title="4.6 推理"></a>4.6 推理</h3><p>&emsp;&emsp;  在上一节中，我们提到了在Demonstration 构造中，有一类是面向<strong>困难的</strong>数理逻辑任务的CoT（Chain of Thought）Prompting，它主要面向较困难的知识推理、数理推理任务，因为对于此类任务，单纯提升模型收效甚微，需要研究通过其他手段来挖掘LLM在推理任务的能力。<br>CoT提供给模型一系列中间的自然语言推理步骤，类似于人类演算的过程，进而得到最终的输出，使得模型不仅要知其然，且要知其所以然；图14是CoT的输入形式。</p>
<img src="/2023/01/01/大型语言模型(LLM)的使用和思考/p14.png" width=65%>
<br>
<center>图14  CoT构造形式</center>

<p>&emsp;&emsp;  1）Emergent Abilities</p>
<p>&emsp;&emsp;   对于更困难问题的有效解决，开始让LLM变得<strong>“智能化”</strong>；同时，在CoT的研究过程中，研究者们发现了一项神奇的能力 —— Emergent Abilities（涌现能力）。Emergent Abilities是超出scaling law的，当模型达到千亿时，模型效果突然“跃升”，在图15中可以明显看到。</p>
<img src="/2023/01/01/大型语言模型(LLM)的使用和思考/p15.png" width=65%>
<br>
<center>图15  模型效果“跃升”</center>
<br>
&emsp;&emsp;  而Emergent Abilities只存在于超大规模模型中，且不是所有的大参数模型均存在，甚至连1370亿参数量的LaMDA模型也没有这样的能力，如图16所示；在我们的实验和外部文献中，1750亿的Bloom暂时也未发现此项能力。
<br>
<br>
<img src="/2023/01/01/大型语言模型(LLM)的使用和思考/p16.png" width=65%>
<br>
<center>图16  不同大模型效果评价</center>

<p>&emsp;&emsp;  关于Emergent Abilities 的介绍和剖析，推荐参考文献3的blog。阅读后你会深刻理解Emergent Abilities 出现为何激动仁心，其对于<strong>复杂推理</strong>、<strong>知识推理</strong>和<strong>超强的集外泛化</strong>能力，均会为你开启“新”的世界。</p>
<p>&emsp;&emsp;  此项能力的出现还需要进一步探索，目前证实存在这种强推理能力的，只有GPT-3.5系列包括text-davinci-002(InstructGPT) 和code-davinci-002 (Codex) 以及在其基础上的演进模型（比如ChatGPT）；另外结合指令学习的PALM模型系列也有此项能力。<br>Yao Fu合理推测千亿模型要存在Emergent Abilities 大致有三要素：</p>
<p>&emsp;&emsp;   ● 指令学习：InstructGPT（GPT-3 text-davinci-002）和PaLM均有此任务设置；</p>
<p>&emsp;&emsp;  ● 代码微调：Codex和PaLM均在代码上有过微调；</p>
<p>&emsp;&emsp;   ● CoT微调：进行CoT数据微调后，LLM倾向于拥有Emergent Abilities。</p>
<p>&emsp;&emsp;   2）Reasoning with prompting研究命题</p>
<p>&emsp;&emsp;  回到CoT Prompt本身的讨论，我们可以从表3中得出业界对于此方向的研究命题，包括Prompt 工程设计、推理过程的优化、外源引擎（主要指代码引擎）以及知识增强（内外源知识）。</p>
<img src="/2023/01/01/大型语言模型(LLM)的使用和思考/p16-2.png" width=65%>
<br>
<center>表2  Reasoning with prompting的不同研究话题</center>

<p>&emsp;&emsp;   对Reasoning with LLM 的研究感兴趣的同学具体可以参考文献5，表2也从此文中而来。</p>
<p>&emsp;&emsp;  另外，其他对于LLM的使用，还包括对于其<strong>知识的存储机制、挖掘和利用</strong>的工作，研究者们将LLM作为一个知识库进行知识挖掘，或者探索LLM中知识的热更新，但总体这部分应用不够，且限制较多（受schema限制），此处不作详细阐述。</p>
<p>&emsp;&emsp;  以上对于LLM使用方式的分类和总体脉络，参照Danqi Chen的COS 597G (Fall 2022): Understanding Large Language Models课程，路径见参考文献6；对LLM感兴趣的同学，强烈推荐此门课程的课件学习和相关paper阅读，当然遗憾的是没有看到课程相关video，如果有可以找到的同学欢迎分享。</p>
<h1 id="5-LLM的机遇与挑战"><a href="#5-LLM的机遇与挑战" class="headerlink" title="5. LLM的机遇与挑战"></a>5. LLM的机遇与挑战</h1><p>&emsp;&emsp;  LLM的出现使得机遇与挑战并存，机遇在于技术革新势必会带来<strong>新赛道的机遇</strong>，挑战在于革新也会使越来越多old school方式被抛弃，跟不上节奏落伍的风险将会加大。</p>
<p>&emsp;&emsp;  1）  从<strong>技术演进</strong>角度来说，LLM可能使NLP形成“大一统”之势。如果说BERT让大部分中间任务基本消亡，NLP传统技艺逐渐被替代，那么LLM则会让很多NLP子领域不再具备独立研究价值，它们均会被纳入LLM技术体系；这对于相关长期从事某些子领域的研究者和从业者是一件可怕的事，累积了一定时间的子领域专家经验/技巧trick，一夜即被“暴力”的LLM击败，对相关业务和从业者挑战巨大；</p>
<p>&emsp;&emsp;  但另一个角度来说，当NLP整体能力到达一定阶段时，“大一统”是必然结果，之前划分较多子领域是因为没有强能力模型而需要分而治之，积极利用LLM拓展自身业务的可能性是机遇所在。</p>
<p>&emsp;&emsp;   2）从LLM应用角度来说，LLM-as-a-Service 会越来越普遍，OpenAI 提供的 LLMaaS 服务已经具备较高的速度，并开始逐步探索行之有效的盈利模式，这也是<strong>下游产品的机遇</strong>。截止2021年7月，全球有300多个app在使用GPT3技术，结合gpt3demo网站的数据，其收录了158个基于GPT-3的应用；LLM使得业界能力下限提升，行业门槛下降，业务优势会聚焦在垂直领域的<strong>数据积累资源</strong>。但LLMaaS的盈利模式并不成熟，尚待检验，合理的模式应该需要涉及用户分层，而非全量用户的铺展，这些均增添了较多不确定性，此为应用层面的挑战。</p>
<p>&emsp;&emsp;  3）从推理成本来说，配合LLM的模型压缩、前向加速等手段均可以降低推理成本，虽然下游产品推理的服务成本尚高，但其实作为百亿模型，完成大量高智能任务具备初步可行性，此为机遇；但在降本增效的大环境下，真正将LLM投入生产的挑战性极大，对于LLM短期在生产环境下最实际的用途主要聚焦在线下，主要围绕数据扩增、减少标注成本和数据生产。</p>
<p>&emsp;&emsp;  4） 从训练投入成本来说，作为最限制LLM发展的因素，它也在不断进步，除去本身模硬件升级、模型蒸馏、加速训练技术之外，LLM的稀疏化也会持续发展，SparseGPT应该是其中有代表性的工作之一，此为机遇；当然这样的进步相比高额的投入并不够，所以在第三节中提到的对于LLM的投入，是和相关组织的技术战略相关的。在LM基建层面，目标成为何Level的公司，会影响相关的投入，但LLM绝对是具备<strong>高战略价值的投入</strong>。</p>
<p>&emsp;&emsp;  5） 从国内产研角度来说，这一点其实是比较让人忧虑的；因为LLM过于高昂的成本和苛刻的使用条件，这两年国内对于LLM的研究<strong>成果较少</strong>（累积参数的大模型有，但实际有影响的成果不多），与国外差距在增大。表1可以看到，GPT3后已经更新了5至6代，而国内甚至还没有真正意义上可以匹敌GPT3的基建模型，甚至60亿到130亿的 InstructGPT能力已经超过国内大部分的所谓大模型（当然OpenAI领先太多，其也超出Google的一般大模型，除了PaLM）。</p>
<p>&emsp;&emsp;  当LLM逐渐成为垄断能力，当OpenAI、Google、DeepMind逐渐闭源时，基建模型又会成为“卡脖子”的能力，ChatGPT只是这一阶段的开始。<br>所以笔者认为对于LLM的态度，仅从个人来说，战略上务必重视它，积极应对挑战，不用过分悲观，寻求并抓住LLM带来的机遇；基于以上方法利用好可用的LLM，可控成本下优化自身业务，同时紧跟业界研究方向，“借好”LLM带来的新东风。</p>
<p>&emsp;&emsp;  注：这部分关于LLM的影响和开放讨论网上信息繁多，感兴趣的同学可以阅读文献7和文献8，均有比较多的讨论，其中文献8是昨天看到的，这篇文章信息比较多，逻辑线稍微乱一些，但整体输出了很多有效观点，有兴趣详细了解LLM的同学也可以参考。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ol>
<li><a target="_blank" rel="noopener" href="https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1">How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources</a></li>
<li><a target="_blank" rel="noopener" href="https://aclanthology.org/2022.acl-long.556.pdf">Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity</a></li>
<li> <a target="_blank" rel="noopener" href="https://yaofu.notion.site/A-Closer-Look-at-Large-Language-Models-Emergent-Abilities-493876b55df5479d80686f68a1abd72f">A Closer Look at Large Language Models Emergent Abilities</a></li>
<li> <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2005.14165.pdf">Language Models are Few-Shot Learners</a></li>
<li> <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2212.09597.pdf">Reasoning with Language Model Prompting</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cs.princeton.edu/courses/archive/fall22/cos597G/">COS 597G: Understanding Large Language Models</a></li>
<li> <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/575391861/answer/2834072642">ChatGPT 印证了模型大一统的可行性，这在未来五年会对 NLP 从业者带来怎样的冲击？ - 知乎</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/597586623?utm_campaign=shareopn&amp;utm_medium=social&amp;utm_oi=37478916423680&amp;utm_psn=1595705313004679168&amp;utm_source=wechat_session&amp;s_r=0">通向AGI之路：大型语言模型（LLM）技术精要 - 知乎</a></li>
<li> <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2301.00234.pdf">A Survey for In-context Learning</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.08691.pdf">The Power of Scale for Parameter-Efficient Prompt Tuning</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2012.15723.pdf">Making Pre-trained Language Models Better Few-shot Learners</a></li>
<li> <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2001.07676v3.pdf">Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference</a></li>
</ol>
<h1 id="附录1：LLM中英术语-概念对照表"><a href="#附录1：LLM中英术语-概念对照表" class="headerlink" title="附录1：LLM中英术语/概念对照表"></a>附录1：LLM中英术语/概念对照表</h1><table>
<thead>
<tr>
<th align="left">英文</th>
<th align="left">中文</th>
<th align="left">释义</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Emergent Ability</td>
<td align="left">突现能力</td>
<td align="left">小模型不显现能力，当模型大到一定程度发生质变，突然出现的能力</td>
</tr>
<tr>
<td align="left">Prompt</td>
<td align="left">提示词</td>
<td align="left">将prompt 输入给大模型，大模型会给出相应的completion</td>
</tr>
<tr>
<td align="left">In-context Learning</td>
<td align="left">上下文学习</td>
<td align="left">在 prompt中给大模型提供几个例子，模型即可按照例子做生成</td>
</tr>
<tr>
<td align="left">Demonstrations</td>
<td align="left">/</td>
<td align="left">上下文学习中构造的输入输出对</td>
</tr>
<tr>
<td align="left">Calibration</td>
<td align="left">校准/矫正</td>
<td align="left">针对模型LLM本身存在的bias做概率矫正</td>
</tr>
<tr>
<td align="left">Instruction Tuning</td>
<td align="left">指令微调</td>
<td align="left">用 Instruction 指令来 fine-tune 大模型</td>
</tr>
<tr>
<td align="left">Code Tuning</td>
<td align="left">在代码上微调</td>
<td align="left">用代码来 fine-tune 大模型</td>
</tr>
<tr>
<td align="left">Reinforcement Learning with <br>Human Feedback (RLHF)</td>
<td align="left">基于人类反馈的强化学习</td>
<td align="left">使用人工结果打分来调整模型</td>
</tr>
<tr>
<td align="left">Chain-of-Thought（CoT）</td>
<td align="left">思维链</td>
<td align="left">写 prompt 时，不仅给出结果，还要将得到结果的步骤一步步写出</td>
</tr>
<tr>
<td align="left">Scaling Laws</td>
<td align="left">缩放法则</td>
<td align="left">模型效果的线性增长，要求模型的大小指数增长</td>
</tr>
<tr>
<td align="left">Alignment</td>
<td align="left">与人类对齐</td>
<td align="left">让机器生成符合人类期望的，符合人类价值观的句子</td>
</tr>
<tr>
<td align="left"><br></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody></table>
<h1 id="附录2：相关大语言模型表"><a href="#附录2：相关大语言模型表" class="headerlink" title="附录2：相关大语言模型表"></a>附录2：相关大语言模型表</h1><table>
<thead>
<tr>
<th align="left">模型</th>
<th align="left">参数量</th>
<th align="left">训练数据量</th>
<th align="left">方法和结论</th>
<th align="left">结果示例</th>
<th align="left">文献</th>
</tr>
</thead>
<tbody><tr>
<td align="left">GPT3</td>
<td align="left">0.1B~175B</td>
<td align="left">约500B tokens</td>
<td align="left">Transformer Decoder</td>
<td align="left"><img src="/2023/01/01/大型语言模型(LLM)的使用和思考/p17.png" width=65%></td>
<td align="left">Language Models are Few-Shot Learners</td>
</tr>
<tr>
<td align="left">LaMDA</td>
<td align="left">137B</td>
<td align="left">1.56T words</td>
<td align="left">Transformer Decoder<br>三大目标：质量、安全和根基性（事实正确性）。<br>质量分为合理性、特异性和趣味性；<br>主要根据以上评测指标来约束生成，将生成和排序融合到一起，同时增加了两个任务来融入知识（输入对话上下文，输出知识查询语句；输入知识查询语句，输出生成的最终结果）</td>
<td align="left"><img src="/2023/01/01/大型语言模型(LLM)的使用和思考/p18.png" width=65%></td>
<td align="left">LaMDA: Language Models for Dialog Applications</td>
</tr>
<tr>
<td align="left">WebGPT</td>
<td align="left">760M、13B、175B</td>
<td align="left">Demonstraions: 6209<br>Comprisons:<br>21548</td>
<td align="left">其核心思想是使用GPT3模型强大的生成能力，学习人类使用搜索引擎的一系列行为，通过训练奖励模型来预测人类的偏好，使WebGPT可以自己搜索网页来回答开放域的问题，而产生的答案尽可能满足人类的喜好。</td>
<td align="left"><img src="/2023/01/01/大型语言模型(LLM)的使用和思考/p19.png" width=65%></td>
<td align="left">WebGPT: Browser-assisted question-answering with human feedback</td>
</tr>
<tr>
<td align="left">FLAN-T5</td>
<td align="left">540B</td>
<td align="left">1800个任务</td>
<td align="left">任务的指令 与数据进行拼接。统一的输入输出格式（4种类型），引入chain-of-thought，大幅提高任务数量，大幅提高模型体积；</td>
<td align="left">通过基于指令的微调（flan）可以大幅度提高语言模型的效果；<br> 模型越大效果越好；<br>任务越多效果越好；<br>混杂CoT相关的任务很重要</td>
<td align="left">Scaling Instruction-Finetuned Language Models</td>
</tr>
<tr>
<td align="left">Sparrow<br>（Chinchilla）</td>
<td align="left">70B</td>
<td align="left">/</td>
<td align="left">核心为从人类反馈中学习，创造更安全的对话助手。</td>
<td align="left">将目标分解为详细的规则，并允许智能体引入外部知识来拓宽它可以正确讨论的主题。<br>在78%的时间内正确引用事实性问题的证据，并在对抗性条件下将规则违反率降低到8%。</td>
<td align="left">Improving alignment of dialogue agents via targeted human judgements</td>
</tr>
<tr>
<td align="left">Gopher</td>
<td align="left">44M~ 280B</td>
<td align="left">10.5TB</td>
<td align="left">堆参数的大模型</td>
<td align="left">11/19任务好于GPT3<br><img src="/2023/01/01/大型语言模型(LLM)的使用和思考/p20.png" width=65%></td>
<td align="left">Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher</td>
</tr>
<tr>
<td align="left">RETRO<br>(Retrieval Transformer)</td>
<td align="left">172M~7.5B</td>
<td align="left">/</td>
<td align="left">以 Gopher为基础改进语言模型架构，降低了训练资源成本，并检索增强。<br>在只使用4%的参数量的基础上，RETRO模型获得了与Gopher和 Jurassic-1 模型相当的性能，在大多数测试集上表现优异。</td>
<td align="left"><img src="/2023/01/01/大型语言模型(LLM)的使用和思考/p21.png" width=65%></td>
<td align="left">Improving language models by retrieving from trillions of tokens</td>
</tr>
<tr>
<td align="left">PaLM</td>
<td align="left">8B、62B、540B</td>
<td align="left">780B tokens 包括网页、书籍、维基百科、代码、社交对话</td>
<td align="left">Transformer Decoder</td>
<td align="left">强大的小样本学习能力；<br>在推理任务上有突破表现；</td>
<td align="left">PaLM: Scaling Language Modeling with Pathways</td>
</tr>
<tr>
<td align="left">InstructGPT</td>
<td align="left">1.3B、6B、175B</td>
<td align="left">微调数据1w+，Reward Model 4w+，PPO无标注数据4w+</td>
<td align="left">GPT3.5 Finetune+RLHF指令微调<br></td>
<td align="left">遵循人类指令，零/少样本生成能力强</td>
<td align="left">Training language models to follow instructions with human feedback</td>
</tr>
<tr>
<td align="left">ChatGPT</td>
<td align="left">/</td>
<td align="left">推测和InstructGPT差不多</td>
<td align="left">GPT3.5 （codex基础上）Finetune+RLHF+解决对齐问题</td>
<td align="left">遵循人类指令，拒绝知识范围外问题，零/少样本生成能力强；具有复杂推理能力</td>
<td align="left">/</td>
</tr>
<tr>
<td align="left"><br></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left"><br></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody></table>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://ryanran92.github.io/2023/01/01/%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B(LLM)%E7%9A%84%E4%BD%BF%E7%94%A8%E5%92%8C%E6%80%9D%E8%80%83/" data-id="clg619vr20002s4pa5t1m1nrq" data-title="大型语言模型(LLM)的使用和思考" class="article-share-link">Teilen</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-ChatGPT技术解构与思考" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/12/08/ChatGPT%E6%8A%80%E6%9C%AF%E8%A7%A3%E6%9E%84%E4%B8%8E%E6%80%9D%E8%80%83/" class="article-date">
  <time class="dt-published" datetime="2022-12-07T16:00:00.000Z" itemprop="datePublished">2022-12-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/12/08/ChatGPT%E6%8A%80%E6%9C%AF%E8%A7%A3%E6%9E%84%E4%B8%8E%E6%80%9D%E8%80%83/">ChatGPT技术解构与思考</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="更新日志"><a href="#更新日志" class="headerlink" title="更新日志"></a>更新日志</h1><hr>
<p>2023-03-15<br><br> - 结合文献28~31，增加2.2.5中 <strong>GPT4最新进展</strong>和升级，关于GPT4更详细内容可以移步：<a href="https://ryanran92.github.io/2023/03/16/%E5%85%B3%E4%BA%8EGPT-4%E7%9A%84%E5%8D%81%E9%97%AE%E5%8D%81%E7%AD%94/">关于GPT-4的十问十答</a> </p>
<p>2023-03-14<br><br> - 增加2.2.5 中，结合文献23，增加涌现能力<strong>存在于各个学科</strong>，以及<strong>观测会影响其体现</strong>的观点<br><br> - 增加2.2.5 中，关于 <strong>加大模型规模</strong> 和 <strong>研究人类反馈</strong> 两条路径对比和说明<br><br> - 结合文献25甲子光年的报告，增加3.1 <strong>场景应用</strong>图、3.4<strong>信息革命</strong>图、3.5国内<strong>大模型现状</strong>图<br><br> - 增加3.4中<strong>Kosmos-1</strong>图<br><br> - 结合ChatGPT API后涌现的应用，增加3.1 <strong>应用案例</strong>图</p>
<p>2023-03-04<br><br> - 增加2.2.5 中，GPT3.5前发展进程的图表简述；增加ChatGPT「<strong>自身能力的认知</strong>」的进化观点，参考邱老师讲座<br><br> - 更新3.1 第4点中，「自身升级」中New Bing的升级<br><br> - 增加3.2 第4点「<strong>安全和监管</strong>」内容，融合部分面向安全线分享的调研内容<br><br> - 新增3.4 「未来」内容<br><br> - 更新 ChatGPT API和成本相关内容<br><br> - 其他：部分缩进格式修正</p>
<p>2023-02-21<br><br> - 新增3.5 「写在最后」内容</p>
<p>2023-02-15<br><br> - 新增2.2.6 「从ChatGPT的成功看<strong>大型语言模型的构建思路</strong>」，融合文献16的观点</p>
<p>2023-02-11<br><br>- 增加2.2.5中，ChatGPT的<strong>形成顺序图</strong>，并结合 <a href="https://ryanran92.github.io/2023/01/01/%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B(LLM)%E7%9A%84%E4%BD%BF%E7%94%A8%E5%92%8C%E6%80%9D%E8%80%83/">大型语言模型(LLM)的使用和思考</a>  中大模型对推理能力的学习，推测ChatGPT涌现能力的来源<br><br>- 增加3.3「借鉴和使用」中，<strong>预训练层面利用</strong>的阐述</p>
<p>2023-02-06<br><br>- 新增文章写作出发点的<strong>相关前言</strong></p>
<p>2022-12-22<br><br>- 新增2.2.5 ChatGPT的<strong>进化历程</strong><br><br>- 新增附录1-中英文<strong>术语对照表</strong></p>
<p>2022-12-08<br><br>- 修正2.1章节中对于ChatGPT推测的<strong>训练数据量级</strong><br><br>- <strong>发表</strong>文章</p>
<hr>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>&amp;ensp ; &amp;ensp ;本文首发在腾讯云开发者公众号 <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/QA8ZOtCDP1X2EKzpZCY0RA">https://mp.weixin.qq.com/s/QA8ZOtCDP1X2EKzpZCY0RA</a> 、知乎号 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/591122595">https://zhuanlan.zhihu.com/p/591122595</a> 号中，发表为最初版本，此文进行实时更新，持续增加新的认知；该文获得司内年度知识奖。</p>
<hr>
<h1 id="1-ChatGPT简介"><a href="#1-ChatGPT简介" class="headerlink" title="1.ChatGPT简介"></a>1.ChatGPT简介</h1><p>###1.1 ChatGPT是什么</p>
<p>&amp;ensp ; &amp;ensp ;ChatGPT本质是一个<strong>对话模型</strong>，它可以回答日常问题、进行多轮闲聊，也可以承认错误回复、挑战不正确的前提，甚至会拒绝不适当的请求，在<strong>去除偏见和安全性</strong>上不同于以往的语言模型。ChatGPT从闲聊、回答日常问题，到文本改写、诗歌小说生成、视频脚本生成，以及编写和调试代码均展示了其令人惊叹的能力。</p>
<p>&amp;emsp ;&amp;emsp ;在上周公布博文和试用接口后，ChatGPT很快以令人惊叹的对话能力“引爆”网络，本文主要从技术角度，通过<strong>解构ChatGPT</strong>背后涉及的技术工作，来阐述其如此强大的原因；同时深入思考其对我们目前的<strong>实际工作和方法论的改变</strong>，包括可复用和可借鉴之处。</p>
<h3 id="1-2-ChatGPT的技术背景"><a href="#1-2-ChatGPT的技术背景" class="headerlink" title="1.2 ChatGPT的技术背景"></a>1.2 ChatGPT的技术背景</h3><p>&emsp;&emsp;ChatGPT目前未释出论文文献，仅释出了介绍博文和试用demo，但从博文中提供的技术点和示意图，可以看出与年初公布的InstructGPT 核心思想一致，其关键能力来自三个方面：强大的<strong>基座大模型能力</strong>（InstructGPT），<strong>高质量的真实数据</strong>（干净且丰富），<strong>稳定的强化学习</strong>（PPO算法）。<br>以上是ChatGPT成功的三个要素，具体将在文中第2部分详细展开。</p>
<h3 id="1-3-ChatGPT的主要特点"><a href="#1-3-ChatGPT的主要特点" class="headerlink" title="1.3 ChatGPT的主要特点"></a>1.3 ChatGPT的主要特点</h3><p>&emsp;&emsp;<strong>ChatGPT的优点令人惊叹：</strong></p>
<p>&emsp;&emsp;1）** 强大的语言理解和生成系统**；对话能力、文本生成能力、对不同语言表述的理解均很出色，以对话为载体可以回答多种多样的日常问题（同时对于多轮对话历史的记忆能力和篇幅增强）：</p>
<img src="/2022/12/08/ChatGPT技术解构与思考/p1.png" width=65%>
<img src="/2022/12/08/ChatGPT技术解构与思考/p2.png" width=65%>
<img src="/2022/12/08/ChatGPT技术解构与思考/p3.png" width=65%>


<p>&emsp;&emsp;2）<strong>全面的回答</strong>和渊博的知识；与GPT3等大模型相比，ChatGPT回答更加全面，可以多角度全方位进行回答和阐述，相较以往的大模型，知识被“挖掘”的更充分；</p>
<ul>
<li>直接给出一篇多角度、全方位的提纲：</li>
</ul>
<img src="/2022/12/08/ChatGPT技术解构与思考/p4.png" width=65%>


<ul>
<li>对于一个问题，多角度展开：<img src="/2022/12/08/ChatGPT技术解构与思考/p5.png" width=65%></li>
</ul>
<p>&emsp;&emsp;3）<strong>降低人类学习成本</strong>和节省时间成本；可以满足人类大部分日常需求，比如快速为人类改写确定目标的文字、大篇幅续写和生成小说、快速定位代码的bug等；</p>
<ul>
<li><p>目标改写：</p>
<img src="/2022/12/08/ChatGPT技术解构与思考/p6.png" width=65%></li>
<li><p>续写小说（当然续写对于普通LM也不是难事，“一本正经的胡说八道”）：</p>
</li>
</ul>
<img src="/2022/12/08/ChatGPT技术解构与思考/p7.png" width=65%>

<ul>
<li>代码debug:</li>
</ul>
<img src="/2022/12/08/ChatGPT技术解构与思考/p8.png" width=65%>
<img src="/2022/12/08/ChatGPT技术解构与思考/p9.png" width=65%>

<p>&emsp;&emsp;4） 具有<strong>安全机制</strong>和去除偏见能力；这类问题在以前的大模型中时常出现，ChatGPT在这两点上增加了过滤处理机制，针对不适当的提问和请求，它可以做出拒绝和“圆滑”的回复。</p>
<ul>
<li>对于违法行为的提问：</li>
</ul>
<img src="/2022/12/08/ChatGPT技术解构与思考/p10.png" width=65%>
<img src="/2022/12/08/ChatGPT技术解构与思考/p11.png" width=65%>


<ul>
<li>对于未知事物的“拒绝”：</li>
</ul>
<img src="/2022/12/08/ChatGPT技术解构与思考/p12.png" width=65%>
<img src="/2022/12/08/ChatGPT技术解构与思考/p13.png" width=65%>

<p>&emsp;&emsp;当然ChatGPT并非十全十美，其<strong>缺点也比较明显</strong>：</p>
<p>&emsp;&emsp;1）简单的逻辑问题错误依旧明显存在，发挥不够稳定（但总体比GPT3好很多），特别在有对话历史时，容易被用户误导而动摇；</p>
<img src="/2022/12/08/ChatGPT技术解构与思考/p14.png" width=65%>

<p>&emsp;&emsp;2）ChatGPT有时会给出看似合理、但并不正确或甚至荒谬的答案。部分答案需要自行甄别才能判断正误，特别当本身用户处于未知状态来咨询模型时，更加无法判断真伪；(强化学习训练期间不会区分事实和错误)</p>
<ul>
<li>对于不知道《观沧海》诗歌的人，很容易被误导。</li>
</ul>
<img src="/2022/12/08/ChatGPT技术解构与思考/p15.png" width=65%>

<p>&emsp;&emsp;ChatGPT使得生产者可以用较低成本增加错误信息，而这一固有缺点已经造成了一些实际影响。编程问答网站 StackOverflow 宣布暂时禁止用户发布来自 ChatGPT 生成的内容，网站 mods 表示：看似合理但实际上错误的回复数量太多，已经超过了网站的承受能力；</p>
<p>&emsp;&emsp;3）<strong>抵抗不安全的prompt能力较差</strong>，ChatGPT对于提问方式比较敏感，一些不安全或有偏见的问题，用户通过改变提问方式即可绕过审核，回答继而会表现出偏见；</p>
<p>&emsp;&emsp;4）<strong>过分猜测用户意图</strong>，主要体现在当用户提问意图不明确时，ChatGPT会猜测用户意图，理想情况应为要求用户澄清；此时当用户意图不明确时，很大概率给出不合适的回复；</p>
<p>&emsp;&emsp;5）<strong>部分回复废话较多</strong>，句式固定；通常过度使用一些常见的短语和句式，这与构造训练数据时，用户倾向于选择更长的回复有关。</p>
<ul>
<li>简单的问一个算式结果，ChatGPT回答较啰嗦：</li>
</ul>
<img src="/2022/12/08/ChatGPT技术解构与思考/p16.png" width=65%>

<hr>
<h1 id="2-ChatGPT的工作原理"><a href="#2-ChatGPT的工作原理" class="headerlink" title="2. ChatGPT的工作原理"></a>2. ChatGPT的工作原理</h1><p>###2.1 ChatGPT的训练过程</p>
<p>&emsp;&emsp;ChatGPT训练过程很清晰，主要分为三个步骤，示意如图所示：</p>
<img src="/2022/12/08/ChatGPT技术解构与思考/p17.png" width=65%>

<p><strong>Step1：</strong></p>
<p>&emsp;&emsp;使用有监督学习方式，基于<strong>GPT3.5</strong>微调训练一个初始模型；训练数据约为2w~3w量级（<strong>感谢goethe同学指正</strong>，此处仅为<strong>推测量级</strong>，是我们根据兄弟模型InstructGPT的训练数据量级估算的，后者详情可参阅<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.02155.pdf">https://arxiv.org/pdf/2203.02155.pdf</a> P33 Table6，真实数据以ChatGPT公布结果为准），由标注师分别扮演用户和聊天机器人，产生人工精标的多轮对话数据；值得注意的是，在人类扮演聊天机器人时，会得到机器生成的一些建议来<strong>帮助人类撰写</strong>自己的回复，以此提高撰写标注效率。</p>
<p>&emsp;&emsp;以上精标的训练数据虽然数据量不大，但<strong>质量和多样性非常高</strong>，且来自真实世界数据，这是很关键的一点。经过第一步，微调过的GPT3.5初步具备了<strong>理解</strong>人类Prompt所包含<strong>意图</strong>的能力，可以根据不同意图给出<strong>高质量的回答</strong>。</p>
<p><strong>Step2：</strong></p>
<p>&emsp;&emsp;收集相同上文下，根据回复质量进行排序的数据：即随机抽取一大批Prompt，使用第一阶段微调模型，产生多个不同回答：$ (P,a_1), (P,a_2), (P,a_3) …  (P,a_k)$ ，之后标注人员对$k$个结果排序，形成$ C_{k}^{2} $组训练数据对，使用pairwise loss来训练奖励模型，从而可以预测出标注者更喜欢哪个输出，”从比较中”学习可以给出相对精确的奖励值。</p>
<p>&emsp;&emsp;这一步使得ChatGPT从命令驱动转向了<strong>意图驱动</strong>，用李宏毅老师的原话，它会不断“<strong>引导GPT说人类要他说的</strong>”。训练数据不需过多，维持在万量级即可，因为它不需要穷尽所有的问题，只是要告诉模型<strong>人类的喜好</strong>，强化模型意图驱动的能力。</p>
<p><strong>Step3：</strong></p>
<p>&emsp;&emsp;使用PPO强化学习策略来微调第一阶段的模型。<strong>核心思想</strong>是随机抽取新的Prompt，用第二阶段的Reward Model给产生的回答打分，这个分数即回答的<strong>整体reward</strong>；进而将此reward回传，由此产生的策略梯度可以更新PPO模型参数；整个过程迭代数次直到模型收敛。</p>
<p>&emsp;&emsp;强化学习算法可以简单理解为通过调整模型参数，使模型得到最大的<strong>奖励</strong>（reward），最大奖励意味着此时的回复最符合人工的选择取向。<br>而对于PPO，我们知道它是2017年OpenAI提出的一种新型的<strong>强化学习策略优化</strong>的算法即可。它提出了新的目标函数，可以在多个训练步骤实现小批量的更新，其实现简单、易于理解、性能稳定、能同时处理离散/连续动作空间问题、利于大规模训练。</p>
<p>&emsp;&emsp;以上三个步骤即ChatGPT的训练过程，合称为文献中提到的** RLHF**(Reinforcement Learning from Human Feedback)技术。</p>
<p>###2.2 ChatGPT为何成功？</p>
<p>&emsp;&emsp;为何三段式的训练方法就可以让ChatGPT如此强大？其实，以上的训练过程蕴含了上文我们提到的关键点，而这些关键点正是ChatGPT成功的原因：</p>
<ul>
<li>强大的<strong>指令学习</strong>能力（InstructGPT）；</li>
<li><strong>大规模参数</strong>语言模型（GPT3.5）；</li>
<li><strong>高质量</strong>的真实数据（精标的多轮对话数据和比较排序数据）；</li>
<li><strong>性能稳定</strong>的强化学习算法（PPO算法）</li>
</ul>
<p>&emsp;&emsp;我们需要注意的是，ChatGPT的成功，是在前期大量工作基础上实现的，非凭空产生的“惊雷”。</p>
<h4 id="2-2-1-InstructGPT"><a href="#2-2-1-InstructGPT" class="headerlink" title="2.2.1 InstructGPT"></a>2.2.1 InstructGPT</h4><p>&emsp;&emsp;ChatGPT是InstructGPT的兄弟模型(sibling model)，后者经过训练以遵循Prompt中的指令，提供详细的响应。InstructGPT是OpenAI在今年3月在Training language models to follow instructions with human feedback文献中提出的工作，整体流程和以上的ChatGPT流程基本相同，除了在<strong>数据收集和基座模型</strong>（GPT3 vs GPT 3.5），以及第三步初始化PPO模型时略有不同。</p>
<p>&emsp;&emsp;此篇可以视为RLHF 1.0的收官之作。一方面，从官网来看，这篇文章之后暂时没有发布RLHF的新研究，另一方面这篇文章也佐证了<strong>Instruction Tuning</strong>的有效性。</p>
<p>&emsp;&emsp;在InstuctGPT的工作中，与ChatGPT类似，给定Instruction，需要人工写回答。首先训练一个InstructGPT的早期版本，使用完全人工标注的数据，<strong>数据分为3类</strong>：Instruction+Answer，Instruction+多个examples和用户在使用API过程中提出的需求。从第二类数据的标注，推测ChatGPT可能用检索来提供多个In-context Learning的示例，供人工标注。剩余步骤与以上ChatGPT相同。</p>
<p>&emsp;&emsp;尤其需要重视但往往容易被忽视的，即OpenAI对于<strong>数据质量和数据泛化性的把控</strong>，这也是OpenAI的一大优势：</p>
<p>&emsp;&emsp;1）寻找<strong>高质量标注者</strong>：寻找在识别和回应敏感提示的能力筛选测试中，表现良好的labeler；</p>
<p>&emsp;&emsp;2）使用集外标注者<strong>保证泛化性</strong>：即用未经历以上1）步骤的更广大群体的标注者对训练数据进行验证，保证训练数据与更广泛群体的偏好一致。</p>
<p>&emsp;&emsp;在完成以上工作后，我们可以来看看InstuctGPT与GPT3的区别，通过下图可以明显看出：</p>
<img src="/2022/12/08/ChatGPT技术解构与思考/p18.png" width=65%>

<p>&emsp;&emsp;GPT3的<strong>回答简短</strong>，回复过于通用毫无亮点；而InstructGPT“侃侃而谈”，解释自由主义为何愚蠢，显然模型学到了对于此类问题人们更想要的<strong>长篇大论</strong>的回答。</p>
<p>&emsp;&emsp;GPT3只是个语言模型，它被用来预测下一个单词，丝毫没有考虑用户想要的答案；当使用代表用户喜好的三类人工标注为微调数据后，1.3B参数的InstructGPT在多场景下的效果超越175B的GPT3：</p>
<img src="/2022/12/08/ChatGPT技术解构与思考/p19.png" width=65%>
<img src="/2022/12/08/ChatGPT技术解构与思考/p20.png" width=65%>


<p>&emsp;&emsp;InstuctGPT的工作具有开创性，它在“解锁”（unlock）和挖掘GPT3学到的海量数据中的知识和能力，但这些仅通过快速的In-context的方式较难获得；可以说，InstuctGPT找到了一种面向主观任务来挖掘GPT3强大语言能力的方式。<br>OpenAI博文中有这样一段原话：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">This technique uses human preferences as a reward signal to fine-tune our models, which is important as the safety and alignment problems we are aiming to solve are complex and subjective, and aren’t fully captured by simple automatic&amp;nbsp;metrics.</span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;当中提到很关键的一点， 当我们要解决的安全和对齐问题是<strong>复杂和主观</strong>，而它的好坏无法完全被自动指标衡量的时候，此时需要用<strong>人类的偏好</strong>来作为奖励信号来微调我们的模型。</p>
<h4 id="2-2-2-InstuctGPT的前序工作-GPT与强化学习的结合"><a href="#2-2-2-InstuctGPT的前序工作-GPT与强化学习的结合" class="headerlink" title="2.2.2 InstuctGPT的前序工作:GPT与强化学习的结合"></a>2.2.2 InstuctGPT的前序工作:GPT与强化学习的结合</h4><p>&emsp;&emsp;再往前回溯，其实在2019年GPT2出世后，OpenAI就有尝试结合GPT-2和强化学习。在NeurIPS 2020的Learning to Summarize with Human Feedback工作中，OpenAI在摘要生成中，利用了从人类反馈中的强化学习来训练。可以从这篇工作的整体流程图中，看出三步走的<strong>核心思想</strong>： 收集反馈数据 -&gt; 训练奖励模型 -&gt; PPO强化学习。</p>
<img src="/2022/12/08/ChatGPT技术解构与思考/p21.png" width=65%>

<p>&emsp;&emsp;RLHF<strong>第一阶段</strong>，针对多个候选摘要，人工排序（这里就体现出OpenAI的钞能力，按标注时间计费，标注过快的会被开除）；<strong>第二阶段</strong>，训练排序模型（依旧使用GPT模型)；<strong>第三阶段</strong>，利用PPO算法学习Policy（在摘要任务上微调过的GPT）。</p>
<p>&emsp;&emsp;文中模型可以产生比10倍更大模型容量更好的摘要效果。但文中也同样指出，模型的成功部分归功于增大了奖励模型的规模，而这需要很大量级的计算资源，训练6.7B的强化学习模型需要320 GPU-days的成本。</p>
<p>&emsp;&emsp;另一篇2020年初的工作，是OpenAI的Fine-Tuning GPT-2 from Human Preferences工作，同样首先利用预训练模型，训练reward模型；进而使用PPO策略进行强化学习，整体步骤初见ChatGPT的雏形。</p>
<img src="/2022/12/08/ChatGPT技术解构与思考/p22.png" width=65%>


<p>&emsp;&emsp;而RLHF（reinforcement learning from human feedback ）的思想，是在更早的2017年6月的OpenAI Deep Reinforcement Learning from Human Preferences工作提出，核心思想是利用人类的反馈，判断最接近视频行为目标的片段，通过训练来找到最能解释人类判断的奖励函数，然后使用RL来学习如何实现这个目标。</p>
<img src="/2022/12/08/ChatGPT技术解构与思考/p23.png" width=65%>

<p>&emsp;&emsp;可以说，ChatGPT是站在InstructGPT以及以上理论的肩膀上完成的一项出色的工作，它们将LLM（large language model）/PTM(pretrain language model)与RL（reinforcement learning)出色结合，证明这条方向可行，同时也是未来还将持续发展的NLP甚至通用智能体的方向。</p>
<h4 id="2-2-3-PPO"><a href="#2-2-3-PPO" class="headerlink" title="2.2.3 PPO"></a>2.2.3 PPO</h4><p>&emsp;&emsp;PPO(Proximal Policy Optimization) 一种新型的Policy Gradient算法（Policy Gradient是一种强化学习算法，通过优化智能体的行为策略来解决在环境中<strong>实现目标</strong>的问题）。我们只需了解普通的Policy Gradient算法对步长十分敏感，但是又<strong>难以选择合适的步长</strong>，在训练过程中新旧策略的的变化差异如果过大则不利于学习。</p>
<p>&emsp;&emsp;而PPO提出了新的目标函数可以在多个训练步骤实现小批量的更新，解决了Policy Gradient算法中步长难以确定的问题。由于其实现简单、性能稳定、能同时处理离散/连续动作空间问题、利于大规模训练等优势，近年来收到广泛的关注，同时也成为OpenAI默认强化学习算法。</p>
<h4 id="2-2-4-WebGPT和CICERO"><a href="#2-2-4-WebGPT和CICERO" class="headerlink" title="2.2.4 WebGPT和CICERO"></a>2.2.4 WebGPT和CICERO</h4><p>&emsp;&emsp;其实近两年，利用LLM+RL以及对强化学习和NLP训练的研究，各大巨头在这个领域做了非常多扎实的工作，而这些成果和ChatGPT一样都有可圈可点之处。这里以OpenAI的WebGPT和Meta的Cicero为例。</p>
<p>&emsp;&emsp;WebGPT是2021年底OpenAI的工作，其核心思想是使用GPT3模型强大的生成能力，<strong>学习人类使用搜索引擎</strong>的一系列行为，通过训练奖励模型来预测人类的偏好，使WebGPT可以自己搜索网页来回答开放域的问题，而产生的答案尽可能满足人类的喜好。</p>
<p>&emsp;&emsp;Cicero是Meta AI上个月发布的可以以人类水平玩文字策略游戏的AI系统， 其同样可以与人类互动，可以使用战略推理和自然语言与人类在游戏玩法中进行互动和竞争。Cicero的核心是由一个<strong>对话引擎</strong>和一个<strong>战略推理引擎</strong>共同驱动的，而战略推理引擎集中使用了RL，对话引擎与GPT3类似。</p>
<img src="/2022/12/08/ChatGPT技术解构与思考/p24.png" width=65%>

<p>&emsp;&emsp;正如Meta原blog中所说：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The technology behind CICERO could one day lead to more intelligent assistants in the physical and virtual worlds.</span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;而以上也是我们未来力求突破的方向和愿景：一个<strong>真正全方位</strong>的<strong>智能</strong>的文字助手。</p>
<p><strong>（12-22更新）</strong></p>
<h4 id="2-2-5-ChatGPT进化历程"><a href="#2-2-5-ChatGPT进化历程" class="headerlink" title="2.2.5  ChatGPT进化历程"></a>2.2.5  ChatGPT进化历程</h4><p>&emsp;&emsp;最近阅读Yao Fu的两篇关于LLM进化历程，以及LLM Emergent Ability （突现能力）相关的blog，启发良多，此处推荐这两篇有价值的blog：<br><a target="_blank" rel="noopener" href="https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1">https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1</a><br><a target="_blank" rel="noopener" href="https://yaofu.notion.site/A-Closer-Look-at-Large-Language-Models-Emergent-Abilities-493876b55df5479d80686f68a1abd72f">https://yaofu.notion.site/A-Closer-Look-at-Large-Language-Models-Emergent-Abilities-493876b55df5479d80686f68a1abd72f</a></p>
<p><strong>（2023-03-15更新）</strong></p>
<p>&emsp;&emsp;GPT1~GPT3主要靠增加模型参数量和数据量，通过<strong>无监督方式</strong>来得到基建语言模型。GPT3.5之后开始聚焦<strong>数据质量、场景和训练方式</strong>，通过有监督方式进行指令学习；最新出炉的GPT4，是在ChatGPT基础上，扩充了训练数据源，包含了正误数学问题、强弱推理、矛盾一致陈述及各种意识形态的数据，整体和ChatGPT区别不大，主体依旧是RLHF；不过<strong>值得注意的一点</strong>是，在<strong>事实性和安全性</strong>问题解决中，后训练过程是提效的关键，单纯GPT-4模型提升不明显，而经过后训练过程即特定信号的RLHF之后，GPT-4在生成模型固有的局限性上改善显著，其他具体细节没有更多公布。</p>
<img src="/2022/12/08/ChatGPT技术解构与思考/p25.png" width=65%>

<p>&emsp;&emsp;GPT4和ChatGPT相比主要的优势是：<br><br>&emsp;&emsp;&emsp;&emsp;● 能力升级：回答更可靠、更有创意并可以处理更细微的指令，这在多类考试测验中以及与各LLM任务比较中得到；<br><br>&emsp;&emsp;&emsp;&emsp;● 风格自有控制：“系统”自发Prompt，让模型可以按照规定风格个任务回复；<br><br>&emsp;&emsp;&emsp;&emsp;● 局限性改善：幻觉问题显著减轻，比最新的 GPT-3.5 模型高 40%；RLHF 训练中加入额外的安全奖励信号（奖励由 GPT-4 的zero-shot分类器提供），不安全内容下降82%；<br><br>&emsp;&emsp;&emsp;&emsp;● 训练行为预测：<strong>此点值得重视</strong>，因为LLM参数量过多，广泛调参不可行，可以用较小模型提前预测训练行为和loss，可以<strong>极大提升训练效率，降低训练成本</strong>；<br><br>&emsp;&emsp;&emsp;&emsp;● 建立LLM测试标准：开源OpenAI Evals，创建和运行基准测试的框架，对GPT-4等模型进行评估，以此进一步帮助模型改进；<br><br>&emsp;&emsp;&emsp;&emsp;● <strong>专用超算</strong>：OpenAI和微软合作，在Azure重建了深度学习堆栈，从头设计了一台专用超级计算机。<br><br>&emsp;&emsp;整体合为下面一张ppt：</p>
<img src="/2022/12/08/ChatGPT技术解构与思考/p26.png" width=65%>

<p>&emsp;&emsp;回到GPT3.5出现之前，此处其实有两条路可以往下走：一是继续加大模型规模；另一条路是更聚焦的训练方式；第一条路<strong>数据</strong>相比算力，反而成为了瓶颈，根据OpenAI 2020的结论，计算预算增加 10 倍，数据集大小应增加约 1.83 倍，模型大小应增加 5.48 倍；Deepmind 2022年Chinchilla的作者发现，数据和模型大小应该按<strong>相等比例</strong>缩放。如按照GPT-3使用来自 Common Crawl 的45 TB数据估算，要训练一个100万亿参数的模型需要 180 PB 数据（下限），而Common Crawl 的整体大小约为12 PB：</p>
<img src="/2022/12/08/ChatGPT技术解构与思考/p27.png" width=65%>

<p>&emsp;&emsp;第一条路显然过于“夸张”，于是OpenAI开始探索第二条路：<strong>更好的训练方式，设计更贴近人类需求的真实任务</strong>，而非纯单词预测；这就是GPT 3.5之后的整体逻辑，而RLHF的方法得到了验证，原作者也提到，<strong>预算分配给人类反馈数据可能比计算更有意义</strong>，我们从InstructGPT中下图也可以看到，经过PPO的1.3B的模型，效果已超出微调的175B的模型，而模型参数减少了100倍；这意味着<strong>在正确类型的数据上进行训练</strong>，比简单地将模型规模扩大（比如扩大100倍）的价值要大得多。</p>
<img src="/2022/12/08/ChatGPT技术解构与思考/p28.png" width=65%>

<p>&emsp;&emsp;此处借用以上文献的表格供大家参考：<br>| 能力| OpenAI模型|训练方法|OpenAI API| OpenAI论文| 近似的开源模型|<br>| :— | :—| :—| :—|:—|:—|<br>| <strong>GPT3系列</strong>|<br>| 语言生成<br>+ 世界知识 <br>+ 上下文学习|GPT-3初始版本<br><strong>大部分的能力已经存在于模型中，尽管表面上看起来很弱。</strong>|语言建模|Davinci|<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.14165">GPT3论文</a> |<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.01068">Meta OPT</a> |<br>|+ 遵循人类的指令<br><strong>+ 泛化到没有见过的任务</strong>|Instruct-GPT初始版本|指令微调|Davinci-Instruct-Beta|<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.02155">Instruct-GPT论文</a> |<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2110.08207">T0论文</a> <br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.11416">Google FLAN论文</a> <br> |<br>|+ 代码理解<br>+ 代码生成|Codex初始版本|在代码上进行训练|Code-Cushman-001|<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.03374">Codex论文</a> |<a target="_blank" rel="noopener" href="https://github.com/salesforce/CodeGen">Salesforce CodeGen</a> |<br>| <strong>GPT3.5系列</strong>|<br>|++ 代码理解<br>++ 代码生成<br>++ <strong>复杂推理 / 思维链</strong><br>+ 长距离的依赖  (很可能)|现在的Codex<br><strong>GPT3.5系列中最强大的模型</strong>|在代码+文本上进行训练<br>在指令上进行微调|Code-Davinci-002 <br>(目前免费的版本 = 2022年12月)|<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.03374">Codex论文</a> | |<br>|++ 遵循人类指令<br>- 上下文学习<br>- 推理能力<br>++ 零样本生成 |有监督的Instruct-GPT <br><strong>通过牺牲上下文学习换取零样本生成的能力</strong>|监督学习版的指令微调**|Text-Davinci-002|<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.02155">InsructGPT论文</a> 有监督部分|<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2110.08207">T0论文</a> <br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.11416">Google FLAN论文</a> <br>|<br>|+ 遵循人类价值观<br>+ 包含更多细节的生成<br>+ 上下文学习<br>+ 零样本生成|经过RLHF训练的Instruct-GPT<br><strong>和002模型相比，和人类更加对齐，并且更少的性能损失|强化学习版的指令微调</strong>|Text-Davinci-003|<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.02155">InsructGPT论文</a>  从人类反馈中学习|<a target="_blank" rel="noopener" href="https://www.deepmind.com/blog/building-safer-dialogue-agents">Deepmind Sparrow</a> <br><a target="_blank" rel="noopener" href="https://github.com/allenai/RL4LMs">AI2 RL4LMs</a> |<br>|++ 遵循人类价值观<br>++ 包含更多细节的生成<br>++ <strong>拒绝知识范围外的问题</strong> <br>++ 建模对话历史的能力<br>– 上下文学习|ChatGPT<br>** 通过牺牲上下文学习的能力换取建模对话历史的能力**|使用对话数据进行强化学习指令微调**|||<a target="_blank" rel="noopener" href="https://www.deepmind.com/blog/building-safer-dialogue-agents">Deepmind Sparrow</a> <br><a target="_blank" rel="noopener" href="https://github.com/allenai/RL4LMs">AI2 RL4LMs</a> |</p>
<p><strong>（2023-02-11更新）</strong></p>
<p>&emsp;&emsp;以上我们可以更清晰的总结为下图（参考文献14提供，此文献可以帮助对于LM了解较少的同学，从0了解ChatGPT的形成）：</p>
<img src="/2022/12/08/ChatGPT技术解构与思考/p29.png" width=65%>

<p>&emsp;&emsp;可以看出ChatGPT强大的涌现能力（模型举一反三、领域外迁移、强泛化能力等），来源于：<br><br>&emsp;&emsp;&emsp;&emsp;● 代码微调：Code-davinci-002模型在代码上有过微调；一种合理的解释是，<strong>「“代码”是一种建立在具备高度抽象性和逻辑性的思维模式下的“语言”」</strong>，海量代码使模型逐渐掌握代码背后的<strong>抽象能力</strong>与<strong>逻辑能力</strong>，函数之间的调用关系本质上是将复杂<strong>问题拆解</strong>为多个小问题来组合解决，引入代码数据来训练模型能够有效提升模型的<strong>思维链能力</strong>，该技术也被认为是打破scaling law的关键，进而涌现出ChatGPT上感受到的“智能”；<br><br>&emsp;&emsp;&emsp;&emsp;● 指令学习：InstructGPT（GPT-3 text-davinci-002）中的supervised instruction tuning；此条通过对比实验得到，因为text-davinci-001没有推理能力，而经过指令学习的 text-davinci-002有较好的推理能力，Google的PaLM也是如此；指令学习构造了更符合<strong>自然语言形式</strong>的训练数据，在提升语义建模能力的同时，也提升了模型在多种<strong>未知下游任务（OOD，out of distribution）</strong>的泛化能力；<br><br>&emsp;&emsp;&emsp;&emsp;● CoT微调（GPT-3 text-davinci-002）进行CoT数据微调后，LLM倾向于拥有Emergent Abilities；ChatGPT没有公开进行过此类的微调，但推测数据中可能有CoT的数据；至少在PaLM关于CoT数据的结论出现后，OpenAI有理由做出参考。<br></p>
<img src="/2022/12/08/ChatGPT技术解构与思考/p30.png" width=65%>

<p><strong>（2023-03-14更新）</strong></p>
<p>&emsp;&emsp;事实上，涌现能力<strong>存在于各个学科</strong>，蚂蚁社群、神经网络、免疫系统、互联网乃至世界经济，但凡一个过程的整体行为远比构成它的部分复杂，均可称为“涌现”现象，用最常见的水固化例子来解释，临界温度时，水液相系统的行为发生急剧变化，水将进入固相（冰，支配系统行为的规律发生了质的变化，LLM也经历了此类质变，此处作为<strong>尺度的是函数</strong>。</p>
<img src="/2022/12/08/ChatGPT技术解构与思考/p31.png" width=65%>

<p>&emsp;&emsp;文献23同时提出另一个观点，此类涌现能力，与我们<strong>观测不够全面</strong>也有关。比如当评估指标不连续，且不提供“接近度”的概念时，涌现能力显现很明显；（非0即1，如击中 vs 不击中）；但如果<strong>增加衡量连续性</strong>（如对比不击中1cm内 vs 不击中1m），会发现随着模型增加，能力的增加是随着参数规律上涨的，非“涌现”：</p>
<img src="/2022/12/08/ChatGPT技术解构与思考/p32.png" width=65%>

<p>&emsp;&emsp;所以，ChatGPT及相关LLM的涌现能力的出现原因和特性，需要<strong>更精细的观测和研究</strong>。</p>
<p><strong>（2023-03-04更新）</strong></p>
<p>&emsp;&emsp;另外一种很有启发性的观点来自于参考文献17及邱锡鹏教授的讲座中，其通过模型<strong>对自身能力的认知</strong>来阐述ChatGPT相比GPT3模型的能力发展。具体可抽象为下图：</p>
<img src="/2022/12/08/ChatGPT技术解构与思考/p33.png" width=65%>

<p>&emsp;&emsp;模型能力分为四个部分：<strong>know knowns</strong>（知道自身掌握的知识，比如我知道自身对文本生成知识比较了解）、<strong>know unknowns</strong>（知道自己没掌握的知识，比如我知道自身对图像生成知识欠缺了解）、<strong>unknow knowns</strong>（不知道自己已经掌握了的知识，比如我触类旁通的掌握了模型压缩知识，但没有实际使用未发觉）、<strong>unknow unknowns</strong>（自身根本不知道有这些知识，掌握更无从谈起；比如我根本不知道三体这个问题的存在，更无从谈对其的了解和认知，此部分一般是<strong>个人认知的最大瓶颈</strong>）</p>
<p>&emsp;&emsp;以上三项来源，显然<strong>CoT</strong>通过思维链能力，发掘了模型自身潜在掌握的知识，即unknow knowns，此部分知晓后扩充至know knowns；而指令学习让模型对自身掌握/未掌握的认知更加明确，即扩大了know knowns和know unknowns的范围；而关于伦理道德等的部分，通过人工反馈学习，主动增加至know unknowns部分；最后此消彼长，<strong>核心在降低unknow unknowns的比重</strong>；可以说，模型和人类一样，<strong>unknow unknowns比例越低，智能化程度越高，对自身的认知也越清晰</strong>。</p>
<p><strong>（0304更新止）</strong></p>
<p>&emsp;&emsp;从OpenAI官方接口中ChatGPT的模型代号为<strong>text-chat-davinci-002</strong>也可以看出其为text-davinci-002的分支，可惜目前text-chat-davinci-002-20221122和text-chat-davinci-002-20230126两个模型都被官方ban掉了。</p>
<p>&emsp;&emsp;ChatGPT的API接口在3月1号“千呼万唤始出来”：调用方式和GPT3.5模型类似：调用Completion类(此处为ChatCompletion）create方法，指定模型名<code>gpt-3.5-turbo</code>即可：<a target="_blank" rel="noopener" href="https://platform.openai.com/docs/guides/chat">https://platform.openai.com/docs/guides/chat</a> ； 让人吃惊的是，<strong>成本仅为GPT3.5模型的1/10</strong>，为每1000个tokens（约750个words）0.002美元，如此相对低廉的成本，让人不禁怀疑如此强大的ChatGPT在RLHF的加成下，公开的版本是否只是百亿模型；或者此次开放的API，是否在<strong>模型加速和成本优化方面</strong>又是一次大的提升。</p>
<p><strong>（2023-02-15更新）</strong></p>
<h4 id="2-2-6-从ChatGPT的成功看大型语言模型的构建思路"><a href="#2-2-6-从ChatGPT的成功看大型语言模型的构建思路" class="headerlink" title="2.2.6 从ChatGPT的成功看大型语言模型的构建思路"></a>2.2.6 从ChatGPT的成功看大型语言模型的构建思路</h4><p>&emsp;&emsp;最近笔者一直在思考，为何ChatGPT经过了代码微调、指令学习以及CoT微调之后，就会有强大的涌现能力（模型举一反三、领域外迁移、强泛化能力等），除了模型本身的千亿量级之外，我们是否能摸清其中的逻辑和基本思路，如果我们要构建自己的ChatGPT，需要<strong>遵从怎样的构建思路</strong>。</p>
<p>&emsp;&emsp;今天阅读参考文献16时，Yao Fu给出了比较高维和系统化的总结：<strong>即大规模语言模型（LLM）的构建，分为四步，分别是「预训练」、「指令微调」、「对齐」、「专门化」。</strong>具体来说（以下为参考文中内容进行的汇总）：</p>
<p>&emsp;&emsp;1）** 预训练：得到强基础模型**；<br><br>&emsp;&emsp;「预训练可以得到语言生成、世界知识、In-context Learning、代码理解/生成、复杂推理/思维链等能力」:</p>
<center><img src="/2022/12/08/ChatGPT技术解构与思考/p34.png" width=65%></center>

<p>&emsp;&emsp;从Yao Fu文中上图可以看出，我们日常使用的开源的Bloom、OPT只属于第一层级，（好奇此处Code-davinci-002为何也属于此层级，因为它已经历过指令学习）；这一层级，是我们之前通常意义上提到的大模型；</p>
<p>&emsp;&emsp;2）** 指令微调：释放模型能力<strong>；<br><br>&emsp;&emsp;「指令微调旨在加强预训练模型的</strong>已有能力<strong>，或者开发出预训练模型</strong>不具备的能力<strong>，指令微调的思路为让模型在各项维度上的</strong>能力全面扩张<strong>。」 文中有个非常有趣的观点，即将不同指令视作线代中的</strong>一组基<strong>，指令微调学到了将这些基进行组合的能力，从而极大的扩张了它的集外泛化能力。这个观点</strong>部分解释**了指令微调带来的质变，首次看到这样的解释，具有启发意味。</p>
<center><img src="/2022/12/08/ChatGPT技术解构与思考/p35.png" width=65%></center>

<p>&emsp;&emsp;此部分我们平时涉及开始变少，其中text-davinci-002/003仅有过调用；此层级也倾向属于模型基建层，self-instruct是一项值得注意的工作：<a target="_blank" rel="noopener" href="https://github.com/yizhongw/self-instruct">https://github.com/yizhongw/self-instruct</a> ，这部分的核心是<strong>需要构造高质量的指令数据</strong>：种类足够多，每个类别下例子足够多，而<strong>中文指令数据现在稀缺</strong>。</p>
<p>&emsp;&emsp;3）** 对齐：与人类价值观匹配**<br></p>
<p>&emsp;&emsp;文中的观点是：“对齐旨在塑造模型的「<strong>价值观</strong>」，使其符合人类的期望，进而塑造模型的「<strong>人格</strong>」。”</p>
<center><img src="/2022/12/08/ChatGPT技术解构与思考/p36.png" width=65%></center>

<p>&emsp;&emsp;到这一步，便是<strong>ChatGPT，以及Calude</strong>之类以<strong>大模型+RLHF</strong>为核心的对话模型，这里需要注意的是，对齐操作<strong>不一定</strong>非要使用强化学习，使用大批量的人工反馈数据的有监督学习，也可以达到类似的效果（至于强化学习能超出多少，此点存疑？）</p>
<p>&emsp;&emsp;4）<strong>模型专门化：从通用到专用</strong><br></p>
<p>&emsp;&emsp;「在经过了预训练、指令微调、对齐操作后，我们进一步考虑对模型进行<strong>专门化处理</strong>，使 ChatGPT 的能力从大学生成长为博士生或教授。」如果做相同的类比，第一阶段的GPT 3.5初始模型/Bloom等相当于九年义务教育、具备<strong>初步通用知识</strong>的人，而后续模型在持续提升专业能力；这里需要注意的是，在进行模型专门化时，同样也需要进行<strong>模型预训练</strong>，接着进行<strong>指令微调</strong>；</p>
<p>&emsp;&emsp;<strong>第四步是我们应用层的机会</strong>，包括我们做的很多任务，均可以在ChatGPT等强能力模型上构造专用的智能模型。非常<strong>推荐</strong>大家详细阅读文献16及其相关文献。</p>
<p>&emsp;&emsp;另外，附录1中增加LLM相关中英文术语和概念（同参考以上文献，但润色相关说法），它们是LLM最近研究相关的核心词，同时这些概念预计也会在未来我们的技术报告中多次涉及，供大家参考。</p>
<hr>
<h1 id="3-ChatGPT应用和思考"><a href="#3-ChatGPT应用和思考" class="headerlink" title="3. ChatGPT应用和思考"></a>3. ChatGPT应用和思考</h1><h3 id="3-1-应用"><a href="#3-1-应用" class="headerlink" title="3.1 应用"></a>3.1 应用</h3><p>&emsp;&emsp;ChatGPT的基础<strong>通用性决定了其应用的广泛性</strong>。这里可以借用AIGC的整体思路，作为“智能入口”的ChatGPT，可以作为<strong>整个AIGC的核心和Foundation model</strong>，接入海量下游任务：（下图来源：量子位、腾讯研究院，甲子智库梳理，2023年）</p>
<center><img src="/2022/12/08/ChatGPT技术解构与思考/p37.png" width=65%></center>


<p>&emsp;&emsp;1）ChatGPT对于<strong>文字模态的AIGC</strong>应用具有重要意义，可以依附于对话形态的产品和载体大有空间，包括但不限于内容创作、客服机器人、虚拟人、机器翻译、游戏、社交、教育、家庭陪护等领域，或许都将是 ChatGPT 能快速落地的方向。</p>
<p>&emsp;&emsp;其中有些方向会涉及到<strong>交互的全面改革</strong>，比如机器翻译不再是传统的文本输入-&gt;实时翻译，而是随时以助手问答的形式出现，甚至给出一个笼统的中文意思，让机器给出对应英文；包括对于我们目前所做的写作产品，可能也会涉及创作模式的改变和革新。有些方向会全面提升产品质量，比如已存在的客服机器人、虚拟人等。</p>
<p>&emsp;&emsp;2）ChatGPT作为文字形态的基础模型，自然可以与<strong>其他多模态结合</strong>；比如最近同为火热的Stable Diffusion或者Midjourney，利用ChatGPT生成较佳的Prompt，对于AIGC内容和日趋火热的艺术创作，提供强大的文字形态的动力。</p>
<p>&emsp;&emsp;3）另一个讨论较多的方向，是ChatGPT对于<strong>搜索引擎的代替性</strong>；ChatGPT可以作为搜索引擎的有效补充，但至于是否能代替搜索引擎（不少人关注的地方），抛开推理成本不谈，目前只从效果上来说为时尚早。</p>
<p>&emsp;&emsp;对于网络有答案的query，抽取就完全能满足，百度最近就有这样的功能；而对于网络上没有明确答案的内容，即使检索了相关材料（ChatGPT应该还没有这样的功能），生成结果的可信度也是一个问题。</p>
<p>&emsp;&emsp;4）ChatGPT<strong>本身的升级</strong>：与WebGPT的结合，对信息进行实时更新，并且对于事实真假进行判断；很明显可以看到，现在的ChatGPT<strong>没有实时更新和事实判断</strong>能力，而如果结合WebGPT的自动搜索能力，让ChatGPT学会自己去海量知识库中探索和学习，将会极大提升使用方向，我们预测这可能会是GPT-4的一项能力。</p>
<p>&emsp;&emsp;在ChatGPT持续升级的3个月内，伴随着New Bing的出现，我们可以看到更高版本的ChatGPT（/GPT-4）已经具有了实时更新和自动搜索的能力，同时给出了参考链接。<strong>ChatGPT本身的升级和进化，使其持续焕发蓬勃的生命力。</strong></p>
<center><img src="/2022/12/08/ChatGPT技术解构与思考/p38.png" width=65%></center>

<p>&emsp;&emsp;ChatGPT API的发布极大提升了各类实际应用的发布速度，未来一到两年，会有源源不断、具有想象力的工具和应用出现。此处借用明日分享的一页ppt，具体也可参考文献26等。</p>
<center><img src="/2022/12/08/ChatGPT技术解构与思考/p39.png" width=65%></center>



<h3 id="3-2-观点"><a href="#3-2-观点" class="headerlink" title="3.2 观点"></a>3.2 观点</h3><p>&emsp;&emsp;参考上文所述，以及参阅近2年OpenAI GPT语言模型相关的文章，RLHF的方法效果显著，ChatGPT成功的核心也在于基于<strong>LLM</strong>（Large language model）的<strong>RLHF</strong>（Reinforcement Learning from Human Feedback），可以说，RLHF是一个很有希望且有趣的方向；强化学习在即将发布的GPT-4中大概率扮演这关键角色。</p>
<p>&emsp;&emsp;结合对于ChatGPT的看法，我们从算法数据、行业创新、核心风险做出了阐述：</p>
<p>&emsp;&emsp;1）首先对于<strong>ChatGPT的规模</strong>，目前没有更多信息支撑，所以无法明确如此智能的ChatGPT是在何规模下达成的。<br>最早的175B的GPT-3代号是Davinci，其他大小的模型有不同的代号。然而自此之后的代号几乎是一片迷雾，不仅没有任何论文，官方的介绍性博客也没有。OpenAI称text-davinci-002/003是GPT-3.5，而它们均为InstrucGPT类型的模型，ChatGPT是基于其中一个微调模型得到，固由此推测<strong>ChatGPT是千亿模型</strong>。（<strong>20230211</strong>：根据最新的接口参数，ChatGPT模型名为text-chat-davinci-002，是在text-davinci-002上结合RLHF训练而来，所以ChatGPT确定是千亿模型）</p>
<p>&emsp;&emsp;2）从<strong>数据层面</strong>来说，数据处理不是简单的标注，优秀的数据也是一种<strong>极大的优势</strong>；除去技术上的考量，OpenAI很少开源数据，显然他们在数据上也下了大功夫，训练语料质量和开源的C4或The Pile不能同日而语；</p>
<p>&emsp;&emsp;3)   ChatGPT<strong>不完全算突破式的创新</strong>，而是OpenAI一步一步扎实工作积累得到的几乎理所当然的结果，属于这两年业界发展的成果汇总，某种程度上，<strong>ChatGPT是工程化的胜利。</strong></p>
<p>&emsp;&emsp;大家一般没有机会接触千亿模型（Bloom之前没有开源的千亿模型，GPT-3也是收费的），不了解现在千亿模型的能力边界，对全量微调这个级别的模型也无从估计。以BERT和T5为代表的早期Transformer和现在的大模型已不是一个量级。事实上11月28日OpenAI上新了text-davinci-003几乎没有引起国内的任何讨论，如果ChatGPT（11-30发布）不是免费试用，或许也不会引起这么大的反响。</p>
<p>&emsp;&emsp;同一时期的工作还有Deepmind的<strong>Sparrow</strong>和Google的<strong>LaMDA</strong>，同样以上提到的WebGPT和Cicero也在国内没有太大的水花。这两年LLM发展已经到了此层级，或许因为<strong>成本或者工程化难度</strong>的问题，某种层面上在国内被忽视了。而此次ChatGPT正好找到了好的“曝光点”，一炮而红。</p>
<p>&emsp;&emsp;所以，<strong>一方面</strong>我们要理性看待ChatGPT的成果，但<strong>另一方面</strong>ChatGPT的出现，会将我们的认识和国外先进思想拉到一条线上，我们应该思考如何利用这些令人激动的最新成果，而其中关键是如何找到<strong>适合我们入口的方式</strong>；</p>
<p><strong>（2023-03-04更新）</strong></p>
<p>&emsp;&emsp;4）ChatGPT的核心风险是<strong>安全和监管</strong>问题；ChatGPT自身的缺陷在1.3中有提及，在逻辑性、事实性上具有生成模型的通病；但最核心的风险是安全问题，具体来说，有以下风险：</p>
<img src="/2022/12/08/ChatGPT技术解构与思考/p40.png" width=65%>

<ul>
<li><p><strong>数据泄漏</strong>，主要涉及</p>
<table>
<thead>
<tr>
<th align="left">场景</th>
<th align="center">敏感数据</th>
</tr>
</thead>
<tbody><tr>
<td align="left">协助定位代码bug</td>
<td align="center">部分源代码</td>
</tr>
<tr>
<td align="left">生成sql语句</td>
<td align="center">表格名、表格字段名</td>
</tr>
<tr>
<td align="left">根据数据绘制表格/作图代码</td>
<td align="center">图表中涉及的数据</td>
</tr>
<tr>
<td align="left">破解程序密钥、令牌</td>
<td align="center">开发明文与关联上下文</td>
</tr>
<tr>
<td align="left">此部分在直接调用OpenAI的API时风险较高。</td>
<td align="center"></td>
</tr>
</tbody></table>
</li>
<li><p><strong>内容安全</strong>，主要涉及虚假信息、错误知识、有害内容生成：</p>
</li>
</ul>
<img src="/2022/12/08/ChatGPT技术解构与思考/p41.png" width=65%>

<ul>
<li><strong>攻击武器</strong>，涉及到有害脚本等攻击脚本的自动生成，其潜在风险是<strong>降低准入门槛、扩大恶意黑客</strong>的群体规模：</li>
</ul>
<img src="/2022/12/08/ChatGPT技术解构与思考/p42.png" width=65%>
 
<ul>
<li><strong>知识产权</strong>，包括自动写作、音乐创作等AIGC内容的产权归属模糊，尚需法律法规界定。</li>
</ul>
<p>&emsp;&emsp;不过值得注意的一点是，ChatGPT在<strong>安全层面“不断进化”</strong>，在笔者构造不安全case时，在0213 version上“绕开成本”大幅增加，需要5~6轮“催眠”和“假设”才能绕开ChatGPT的安全限制，此前很轻易就能绕开。</p>
<p> &emsp;&emsp;面对以上的风险，<strong>立法探索和监管</strong>势在必行；其次结合机器手段，维护训练/精调<strong>语料库清朗</strong>环境是数据层级的手段；而在<strong>技术层面</strong>，不管是生成前“魔法”打败“魔法”的Prompt Evalutor，生成时的self-correction；还是生成后的鉴别检测（水印、数学特性、分类判别等），皆是不断提升安全性的手段；最后一点，从长远来看，大模型的风险管理人才也需要培养。</p>
<h3 id="3-3-借鉴和使用"><a href="#3-3-借鉴和使用" class="headerlink" title="3.3 借鉴和使用"></a>3.3 借鉴和使用</h3><p>&emsp;&emsp;对于ChatGPT的借鉴和使用，大致可以归类以下五个方向：</p>
<p><strong>（2023-02-11更新：增加第一点：预训练层面的使用）</strong></p>
<p>&emsp;&emsp;1） <strong>预训练</strong>层面：即<strong>从头预训练（training from scratch），得到自己的ChatGPT</strong>；这是最直接、具有门槛且适合大公司的基础设施建设工作，具有较强的<strong>战略性</strong>；同时随着硬件<strong>利用率上升</strong>和硬件<strong>升级</strong>，训练大模型成本逐年下降，以GPT3的训练成本来说，在两年半时间里，与GPT-3性能相当的模型的训练和推理成本下降了约80%。对于大公司来讲，训练成本已非卡脖子因素，<strong>瓶颈反而在于高质量数据</strong>：「与增加高质量训练数据集的大小相比，增加模型参数的数量能获得的边际收益越来越小。」</p>
<p>&emsp;&emsp;关于新增此点，笔者之前觉得国内对于从头预训练千亿模型呈不看好之势，但在此文形成的两个月内，国内风向突然发生变化。</p>
<img src="/2022/12/08/ChatGPT技术解构与思考/p43.png" width=65%>

<p>&emsp;&emsp;从1月底开始ChatGPT持续翻火，ChatGPT本身的能力持续被世人所认知：<strong>一方面</strong>，伴随微软和谷歌等巨头在搜索市场的激烈竞争将其推向风口，前者迅速将ChatGPT融入Bing搜索，后者推出了暂时不尽如人意的Bard；<strong>另一方面</strong>，在国内伴随营销号的宣传下其迅速呈出圈之势，各大公司突然涌向此风口，开始要做自己的ChatGPT，抑或宣称已存在自己的ChatGPT，这样的营销/宣传行为无可厚非，至少让国内也开始意识到自身在大模型上的差距，同时进行反思以及向世界看齐。</p>
<p>&emsp;&emsp;在笔者看来，复制ChatGPT对于国内很多头部企业来说<strong>并非最大的难事</strong>，虽然有较多挑战，但最终可能会成功；但我们真正缺的是看待前沿技术的<strong>视野、前瞻性、战略眼光和发展的魄力</strong>，技术需要在做出判断后，有一定的冒险精神去做正确的事情，过于保守的逐利虽然稳妥，但没有持续的创新即意味着在原有事物中持续的“卷”和内耗。在GPT3出现后的2年半内，我们一直都有开始跟进的契机，而绝非随着ChatGPT的出现后才突然觉醒；但为时不晚，至少我们已经启程。</p>
<p>2）** 直接使用**层面：此层面为复用API中效果极佳的部分，根据最新产品同学的调研结果中，可以看出设定较佳的Prompt前提下，可以快速实现高满意度的写作的多层级需求，这也是很让人激动的部分。</p>
<p>&emsp;&emsp;直接使用的<strong>优势</strong>是可以快速实现多粒度多层级上文·功能需求，尤其是很多需求难以定义清晰、数据难以获得的情况下，复用并包装这样的功能一本万利；</p>
<p>&emsp;&emsp;缺点也很明显，最主要是以上提到的安全问题；另一方面，直接调用成本是较高，根据GPT3.5(Davinci)的成本推测:</p>
<img src="/2022/12/08/ChatGPT技术解构与思考/p44.png" width=65%>

<p>&emsp;&emsp;1k tokens≈700 words为0.02美元，则换算后，一篇2k字的文章，直接调用需要0.4人民币，若保守按照日活1w用户，人均10篇文章计算，则每日调用成本为：10000x10x0.4=40000元，成本过于高昂，但实现时间最少；<br>另外，根据Musk Twitter上与OpenAI工作人员的对话，也可以看到每次聊天过程需要几美分的成本，所以ChatGPT直接调用成本较高。</p>
<img src="/2022/12/08/ChatGPT技术解构与思考/p45.png" width=65%>

<p>&emsp;&emsp;当然随着最新OpenAI接口的释出，成本比之前的预测低1/10，但要做出真正适配自身、稳定、高并发的ChatGPT，成本是<strong>高于单纯API调用成本</strong>的。</p>
<p>&emsp;&emsp;3）间接使用层面：此层面核心思想是利用OpenAI接口，按照不同需求生成高质量小样本数据，克服现有数据难获得的瓶颈；进而利用现有Bloom（GPT3模型）进行<strong>数据扩增</strong>，这是目前比较切实，实现时间较少，是在时间成本和效果上折中的方式。</p>
<p>&emsp;&emsp;4）思想借鉴：</p>
<p>&emsp;&emsp;a. 参考RLHF的方法，我们目前有初步尝试，如对多候选进行标注，利用得到的标注结果重新微调生成模型，或者增加排序阶段加入RL学习；</p>
<p>&emsp;&emsp;b. 尝试一些高效调参的方法，<strong>微调</strong>176B的Bloom，但此条受限于资源尚需评估和确认。</p>
<p>&emsp;&emsp;总的来说，将改写从最初的seq2seq，拓展到GPT+Instruction Tuning路径。</p>
<p>&emsp;&emsp;实现时间： （1）&lt; (2) &lt;  (3)</p>
<p>&emsp;&emsp;资源成本： （1）&gt; (3) &gt;  (2)</p>
<p>&emsp;&emsp;5) 交互升级：</p>
<p>&emsp;&emsp;将写作整体打造为ChatBot的形式，此核心思想见另一篇关于对话系统报告中相关性部分，涉及到交互层面的变革；但ChatGPT的出现和核心技术，让形式升级成为可能，而且预计随着深度学习和多智能体系统的发展，未来会有多种多样多功能的X-GPT/X-Bot出现。</p>
<p>&emsp;&emsp;交互升级的核心是提供“<strong>个性化的服务</strong>”，用户可以得到根据自身需求定制的结果，而这些结果是用户通过交互和不同的指令修正反馈给模型，人人的需求都能被个性化地满足。</p>
<h3 id="3-4-未来"><a href="#3-4-未来" class="headerlink" title="3.4 未来"></a>3.4 未来</h3><p>&emsp;&emsp;1) ChatGPT是深层次的<strong>信息革命</strong>，其核心是提升了<strong>信息获取效率</strong>，单以搜索引擎举例，人类过去需要搜索-点击-搜寻-整理，最后得到搜索结论；而随着可信度和实时性的优化，ChatGPT会直接通过交互和修正快速呈现结论，效率的提升本质是<strong>生产力的提升</strong>；基于信息效率的提升，人类会更专注于所谓“更高层次、思想编辑”的工作，未来两年内，人类的“信息搬砖”工作可能越来越少（数据来源：甲子光年智库，2023年）；</p>
<img src="/2022/12/08/ChatGPT技术解构与思考/p46.png" width=65%>

<p>&emsp;&emsp;2) ChatGPT本身还有很多优化点，包括<strong>继续扩大模型参数</strong>：更长的输入框；更大的模型，更多的数据；增加<strong>多模态的信息</strong>，达到多模态智能的统一；包括Visual ChatGPT、Kosmos-1、PaLM-E的出现印证了这一点，实现真正多模态的统一的GPT4很快就会来（图为Kosmos-1）：<br>另一方面<strong>模型的专业化</strong>，垂直类指令的学习；还有包括学会“工具”的使用，即调用外部能力，Meta AI的Toolformer已经开始涉及；</p>
<img src="/2022/12/08/ChatGPT技术解构与思考/p47.png" width=65%>

<p>&emsp;&emsp;3) ChatGPT在带来范式改变，让很多NLP子任务“消失”的同时，也会带来新的研究问题：一条路线是如何精准对大模型提出需求，对ChatGPT进行“催眠”的<strong>Prompt Enginering</strong>，网络上有很多Prompt调教指南：<a target="_blank" rel="noopener" href="https://github.com/PlexPt/awesome-chatgpt-prompts-zh%EF%BC%9B">https://github.com/PlexPt/awesome-chatgpt-prompts-zh；</a> 或Prompt的学习和自动生成；AI生成<strong>内容的检测</strong>延续之前的方向持续发展，虽然已有包括GPT-zero、WaterMark等方法已提出，但目前效果一般；另外就是<strong>Machine Unlearning</strong> ，目标是使得大模型可以有效地保护用户隐私数据，遗忘需要它遗忘的知识；</p>
<p>&emsp;&emsp;4) <strong>超越人类？</strong>此条观点主要来自于文献20，符尧认为ChatGPT类模型在<strong>并行感知</strong>：极短时间内多篇信息输入；<strong>记忆遗传</strong>：模型演化记忆；<strong>加速时间</strong>：模型进化速度可能超出人类；以及<strong>无限生命</strong>：模型权重不会丢失等层级可能会超越人类。</p>
<p>&emsp;&emsp;这一点虽然具有某些理想主义的色彩：“数字永生”，但ChatGPT除了强大的意图理解和生成能力之外，其<strong>进化速度和持续学习</strong>等能力，确实与之前的模型有本质区别；若其进化速度果真超出scaling law，所谓的AGI可能会越来越近；当然笔者认为，归根结底ChatGPT依旧是一个概率模型，没有逻辑处理模块、没有真实记忆模块，能否实现AGI尚需时间检验。</p>
<h3 id="3-5-写在最后"><a href="#3-5-写在最后" class="headerlink" title="3.5 写在最后"></a>3.5 写在最后</h3><p>&emsp;&emsp;做中国自己的ChatGPT是国内讨论火热的话题之一，各大厂已经纷纷押注。如文献18提到，ChatGPT = 50% 数据 + 30% 场景 + 10% 算力 + 10% 团队，国内的优势是<strong>工程化人才的成本、数量和应用规模巨大</strong>，得到国内的ChatGPT难度尚可，但笔者认为，“罗马非一日建成”，单纯复现得到ChatGPT非关键，<strong>核心</strong>是对于战略性的技术、对于卡脖子的基础设施，我们需要具有<strong>发展的眼光，持续关注、持续投入、长远发展</strong>，才能真正具有核心竞争力。</p>
<p>&emsp;&emsp;大模型从算力、数据到模型建立，非一朝一夕之功，需要<strong>集中力量办大事</strong>，企业的责任是一方面，也需要国家的整体战略，从下图可以看出中国的大模型（除了图中的OpenAI相关）基础并不差，战略更加重视之后会有更长足的发展（图来源：HTI，甲子光年智库梳理，2023年）。</p>
<img src="/2022/12/08/ChatGPT技术解构与思考/p48.png" width=65%>

<p>&emsp;&emsp;最后援引川总的一段话作结：</p>
<img src="/2022/12/08/ChatGPT技术解构与思考/p49.png" width=65%>

<p>&emsp;&emsp;希望在“技术的洪流”中我们皆为站在浪尖的弄潮儿，也愿每个技术人都能够实现自己的技术理想。</p>
<hr>
<p>&emsp;&emsp;欢迎大家踊跃讨论ChatGPT相关的技术和思想，有问题随时指正，感谢~</p>
<p>&emsp;&emsp;<strong>注：</strong>本篇报告中，ryanran(冉昱)负责主体框架和内容梳理以及编写，xiamixue(薛晨)在第二章节的工作原理、相关技术以及对于ChatGPT的思考章节提供了有思辨性的思路、观点和文本内容。</p>
<hr>
<h1 id="4-参考文献"><a href="#4-参考文献" class="headerlink" title="4. 参考文献"></a>4. 参考文献</h1><ol>
<li><a target="_blank" rel="noopener" href="https://chat.openai.com/chat">https://chat.openai.com/chat</a> </li>
<li><a target="_blank" rel="noopener" href="https://openai.com/blog/chatgpt/">ChatGPT: Optimizing Language Models for Dialogue</a></li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/570189639/answer/2787763735">如何评价OpenAI的超级对话模型ChatGPT？ - 知乎</a></li>
<li><a target="_blank" rel="noopener" href="https://openai.com/blog/instruction-following/#birds-migrate">Aligning Language Models to Follow Instructions</a></li>
<li> <a target="_blank" rel="noopener" href="https://openai.com/blog/learning-to-summarize-with-human-feedback/">Learning to Summarize with Human Feedback</a></li>
<li>  <a target="_blank" rel="noopener" href="https://openai.com/blog/fine-tuning-gpt-2/">Fine-Tuning GPT-2 from Human Preferences</a></li>
<li><a target="_blank" rel="noopener" href="https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/">Learning from Human Preferences</a></li>
<li><a target="_blank" rel="noopener" href="https://openai.com/blog/openai-baselines-ppo/">Proximal Policy Optimization</a></li>
<li><a target="_blank" rel="noopener" href="https://openai.com/blog/webgpt/">WebGPT: Improving the Factual Accuracy of Language Models through Web Browsing</a></li>
<li><a target="_blank" rel="noopener" href="https://about.fb.com/news/2022/11/cicero-ai-that-can-collaborate-and-negotiate-with-you/">CICERO: AI That Can Collaborate and Negotiate With You | Meta</a></li>
<li>ChatGPT (可能)是怎麼煉成的 - GPT 社會化的過程 <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=e0aKI2GGZNg">https://www.youtube.com/watch?v=e0aKI2GGZNg</a> </li>
<li>A Closer Look at Large Language Models Emergent Abilities  <a target="_blank" rel="noopener" href="https://yaofu.notion.site/A-Closer-Look-at-Large-Language-Models-Emergent-Abilities-493876b55df5479d80686f68a1abd72f">https://yaofu.notion.site/A-Closer-Look-at-Large-Language-Models-Emergent-Abilities-493876b55df5479d80686f68a1abd72f</a></li>
<li>How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources <a target="_blank" rel="noopener" href="https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1">https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1</a></li>
<li>From zero to ChatGPT <a target="_blank" rel="noopener" href="https://xv44586.github.io/2023/01/09/zero-to-chatgpt/">https://xv44586.github.io/2023/01/09/zero-to-chatgpt/</a> </li>
<li> The Economics of Large Language Models <a target="_blank" rel="noopener" href="https://sunyan.substack.com/p/the-economics-of-large-language-models">https://sunyan.substack.com/p/the-economics-of-large-language-models</a></li>
<li>ChatGPT成功做对了这4步丨爱丁堡大学符尧  <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/U5v8CFmGIpjWBheDDWXCPA">https://mp.weixin.qq.com/s/U5v8CFmGIpjWBheDDWXCPA</a></li>
<li> ChatGPT: potential, prospects, and limitations | SpringerLink <a target="_blank" rel="noopener" href="https://link.springer.com/article/10.1631/FITEE.2300089">https://link.springer.com/article/10.1631/FITEE.2300089</a></li>
<li>chatGPT 制胜的关键 <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/yXO4CEq2kMbQtFLLsiaiYA">https://mp.weixin.qq.com/s/yXO4CEq2kMbQtFLLsiaiYA</a></li>
<li>斗象解读：ChatGPT将如何影响网络安全实战攻防 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/609750526">https://zhuanlan.zhihu.com/p/609750526</a></li>
<li>探索智能的极限  <a target="_blank" rel="noopener" href="https://yaofu.notion.site/e1cd16d1fae84f87aeddf872c838e07c">https://yaofu.notion.site/e1cd16d1fae84f87aeddf872c838e07c</a> /</li>
<li>ChatGPT 所帶來的研究問題 <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=UsaZhQ9bY2k">https://www.youtube.com/watch?v=UsaZhQ9bY2k</a></li>
<li>出道即巅峰？国产ChatGPT的风险与应对<a target="_blank" rel="noopener" href="https://km.woa.com/group/induswatchtower/articles/show/535867">https://km.woa.com/group/induswatchtower/articles/show/535867</a></li>
<li>Emergent Abilities of Large Language Models  <a target="_blank" rel="noopener" href="https://www.assemblyai.com/blog/emergent-abilities-of-large-language-models/">https://www.assemblyai.com/blog/emergent-abilities-of-large-language-models/</a></li>
<li>Language Is Not All You Need: Aligning Perception with Language Models <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2302.14045.pdf">https://arxiv.org/pdf/2302.14045.pdf</a></li>
<li>2023中国AIGC市场研究报告：ChatGPT的技术演进、变革风向与投资机会分析 <a target="_blank" rel="noopener" href="https://km.woa.com/asset/5cd39cd314eb4f4f964713e667518a60?height=70&amp;width=765">https://km.woa.com/asset/5cd39cd314eb4f4f964713e667518a60?height=70&amp;width=765</a></li>
<li>ChatGPT的神奇应用 <a target="_blank" rel="noopener" href="https://km.woa.com/asset/d07addd3535f44809f3519119fa2d43f?height=70&amp;width=833">https://km.woa.com/asset/d07addd3535f44809f3519119fa2d43f?height=70&amp;width=833</a></li>
<li>In AI, is bigger always better? <a target="_blank" rel="noopener" href="https://www.nature.com/articles/d41586-023-00641-w">https://www.nature.com/articles/d41586-023-00641-w</a>  <a target="_blank" rel="noopener" href="https://hub.baai.ac.cn/view/24755">https://hub.baai.ac.cn/view/24755</a></li>
<li>GPT-4 <a target="_blank" rel="noopener" href="https://openai.com/research/gpt-4">https://openai.com/research/gpt-4</a></li>
<li>GPT-4震撼发布-机器之心 <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/kA7FBZsT6SIvwIkRwFS-xw">https://mp.weixin.qq.com/s/kA7FBZsT6SIvwIkRwFS-xw</a></li>
<li>GPT-4 is OpenAI’s most advanced system, producing safer and more useful responses <a target="_blank" rel="noopener" href="https://openai.com/product/gpt-4">https://openai.com/product/gpt-4</a></li>
<li>GPT-4 Technical Report <a target="_blank" rel="noopener" href="https://cdn.openai.com/papers/gpt-4.pdf">https://cdn.openai.com/papers/gpt-4.pdf</a></li>
</ol>
<hr>
<h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><ol>
<li>LLM中英术语/概念对照表<strong>（12-22增加）</strong><table>
<thead>
<tr>
<th align="left">英文</th>
<th align="left">中文</th>
<th align="left">释义</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Emergent Ability</td>
<td align="left">涌现/突现能力</td>
<td align="left">小模型不显现能力，当模型大到一定程度发生质变，突然出现的能力</td>
</tr>
<tr>
<td align="left">Prompt</td>
<td align="left">提示词</td>
<td align="left">将prompt 输入给大模型，大模型会给出相应的completion</td>
</tr>
<tr>
<td align="left">In-context Learning</td>
<td align="left">上下文学习</td>
<td align="left">在 prompt中给大模型提供几个例子，模型即可按照例子做生成</td>
</tr>
<tr>
<td align="left">Instruction Tuning</td>
<td align="left">指令微调</td>
<td align="left">用 Instruction 指令来 fine-tune 大模型</td>
</tr>
<tr>
<td align="left">Code Tuning</td>
<td align="left">代码微调</td>
<td align="left">用代码来 fine-tune 大模型</td>
</tr>
<tr>
<td align="left">Reinforcement Learning with <br>Human Feedback (RLHF)</td>
<td align="left">基于人类反馈的强化学习</td>
<td align="left">使用人工结果打分来调整模型</td>
</tr>
<tr>
<td align="left">Chain-of-Thought(CoT)</td>
<td align="left">逻辑链/思维链</td>
<td align="left">写 prompt 时，不仅给出结果，还要将得到结果的步骤一步步写出</td>
</tr>
<tr>
<td align="left">Scaling Laws</td>
<td align="left">缩放法则</td>
<td align="left">模型效果的线性增长，要求模型的大小指数增长</td>
</tr>
<tr>
<td align="left">Alignment</td>
<td align="left">与人类对齐</td>
<td align="left">让机器生成符合人类期望的，符合人类价值观的句子</td>
</tr>
</tbody></table>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://ryanran92.github.io/2022/12/08/ChatGPT%E6%8A%80%E6%9C%AF%E8%A7%A3%E6%9E%84%E4%B8%8E%E6%80%9D%E8%80%83/" data-id="clg619vr00001s4pa2hfd5tbs" data-title="ChatGPT技术解构与思考" class="article-share-link">Teilen</a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archiv</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/03/">March 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/01/">January 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/12/">December 2022</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">letzter Beitrag</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/03/16/%E5%85%B3%E4%BA%8EGPT-4%E7%9A%84%E5%8D%81%E9%97%AE%E5%8D%81%E7%AD%94/">关于GPT-4的十问十答</a>
          </li>
        
          <li>
            <a href="/2023/01/01/%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B(LLM)%E7%9A%84%E4%BD%BF%E7%94%A8%E5%92%8C%E6%80%9D%E8%80%83/">大型语言模型(LLM)的使用和思考</a>
          </li>
        
          <li>
            <a href="/2022/12/08/ChatGPT%E6%8A%80%E6%9C%AF%E8%A7%A3%E6%9E%84%E4%B8%8E%E6%80%9D%E8%80%83/">ChatGPT技术解构与思考</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 Ryanran<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>